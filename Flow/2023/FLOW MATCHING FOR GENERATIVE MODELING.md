论文链接：https://arxiv.org/pdf/2210.02747

代码链接：

# 摘要

我们引入了一种基于 Continuous Normalizing Flows (CNF) 的生成建模新范式，使我们能够以前所未有的规模训练 CNF。具体而言，我们提出了 Flow Matching (FM) 的概念，这是一种无需模拟的 CNF 训练方法，该方法基于固定条件概率路径的回归矢量场。Flow Matching 与一类通用的高斯概率路径兼容，用于在噪声和数据样本之间进行转换——**这将现有的扩散路径归纳为特定实例**。有趣的是，我们发现将 FM 与扩散路径结合使用，可以为训练扩散模型提供一种更稳健、更稳定的替代方案。此外，Flow Matching 为使用其他非扩散概率路径训练 CNF 开辟了道路。一个特别值得关注的例子是使用**最优传输 (OT) 位移插值来定义条件概率路径**。这些路径比扩散路径更高效，可以提供更快的训练和采样速度，并带来更好的泛化效果。在 ImageNet 上使用 Flow Matching 训练 CNF，在似然度和样本质量方面均比其他基于扩散的方法获得更好的性能，并且可以使用现成的**数值 ODE** 求解器快速可靠地生成样本。

# 1.INTRODUCTION

深度生成模型是一类旨在从未知数据分布中进行估计和采样的深度学习算法。近年来，生成模型领域取得了一系列令人瞩目的进展，例如在图像生成领域，这主要得益于基于扩散的模型的可扩展性和相对稳定的训练。然而，由于局限于简单的扩散过程，采样概率路径的空间相当狭窄，导致训练时间非常长，并且需要采用专门的方法来实现高效采样。

在本文中，我们探讨了 Continuous Normalizing Flows (CNF) 的通用且确定性的框架。**CNF 能够建模任意概率路径，尤其以涵盖由扩散过程建模的概率路径而闻名**。然而，除了可以通过例如 denoising score matching 等方法高效训练的扩散之外，尚无已知的可扩展 CNF 训练算法。事实上，最大似然训练（例如 Grathwohl 等人 (2018)）需要昂贵的数值 ODE 模拟，而现有的无需模拟的方法要么涉及难以处理的积分，要么涉及有偏梯度。

本研究的目标是提出 Flow Matching (FM)，这是一种高效的无需模拟的 CNF 模型训练方法，允许采用**通用概率路径**来监督 CNF 训练。重要的是，FM 打破了除扩散之外可扩展 CNF 训练的障碍，并且无需推理扩散过程，直接使用概率路径。

具体来说，我们提出了 Flow Matching 目标（第 3 节），这是一个简单直观的训练目标，用于回归到生成所需概率路径的**目标向量场**。我们首先证明，我们可以通过每个示例（即条件）公式构建此类目标向量场。然后，受 denoising score matching 的启发，我们证明了一个称为 Conditional Flow Matching (CFM) 的基于示例的训练目标，它提供等效梯度，并且不需要明确了解难以处理的目标向量场。此外，我们讨论了**可用于 Flow Matching 的一系列基于示例的概率路径**（第 4 节），它将现有的扩散路径作为特殊实例。即使在扩散路径上，我们也发现使用 FM 可以提供更稳健、更稳定的训练，并且比 score matching 获得更优异的性能。此外，**这一系列概率路径还包含一个特别有趣的情况：对应于最优传输 (OT) 位移插值的向量场**。我们发现，条件 OT 路径比扩散路径更简单，形成直线轨迹，而扩散路径则产生曲线路径。这些特性似乎从经验上转化为更快的训练、更快的生成和更好的性能。

我们在 ImageNet（一个规模庞大且高度多样化的图像数据集）上通过最优传输路径构建 Flow Matching 算法进行了实证验证。我们发现，相比其他基于扩散的竞争方法，我们可以轻松训练模型，使其在似然估计和样本质量方面均取得优异表现。此外，我们发现，与现有方法相比，我们的模型在计算成本和样本质量之间取得了更佳的平衡。图 1 展示了我们模型中选定的 128×128 非条件 ImageNet 样本。

# 2.PRELIMINARIES: CONTINUOUS NORMALIZING FLOWS

令 $\mathbb R^d$ 表示数据空间，其中数据点为 $x = (x^1, ..., x^d) ∈ \mathbb R^d$。本文中使用的两个重要对象是：**概率密度路径** $p: [0, 1] × \mathbb R^d → \mathbb R_{>0}$，它是一个时间相关的概率密度函数，即 $\int p_t(x)dx = 1$；以及时间相关的**矢量场** $v: [0, 1] × \mathbb R^d → \mathbb R^d$。矢量场 $v_t$ 可用于**构建时间相关的微分同胚映射**，称为流 $\phi: [0, 1] × \mathbb R^d → \mathbb R^d$，其通过常微分方程 (ODE) 定义：

$$\frac{d}{dt}\phi_t(x)=v_t(\phi_t(x))\tag{1}$$

$$\phi_0(x)=x\tag{2}$$

此前，Chen et al. (2018) 建议用神经网络 $v_t(x;θ)$ 对矢量场 $v_t$ 进行建模，其中 $θ ∈ \mathbb R^p$ 是其可学习参数，从而得到一个关于流 $\phi_t$ 的深度参数模型，称为 Continuous Normalizing Flow  (CNF)。CNF 用于通过前推方程将简单的先验密度 $p_0$（例如纯噪声）重塑为更复杂的密度 $p_1$。

$$p_t=[\phi_t]_*p_0\tag{3}$$

其中前推（或变量改变）运算符 $∗$ 定义为

$$[\phi_t]_*p_0(x)=p_0(\phi^{-1}_t(x))det[\frac{\partial \phi^{-1}_t}{\partial x}(x)]\tag{4}$$

如果矢量场 $v_t$ 的流 $\phi_t$ 满足方程 3，则称该矢量场生成概率密度路径 $p_t$。测试矢量场是否生成概率路径的一种实用方法是使用连续性方程，它是我们证明中的关键组成部分，请参阅附录 B。我们在附录 C 中回顾了有关 CNF 的更多信息，特别是如何计算任意点 $x ∈ \mathbb R^d$ 处的概率 $p_1(x)$。

# 3.FLOW MATCHING

令 $x_1$ 表示服从某个未知数据分布 $q(x_1)$ 的随机变量。我们假设我们只能访问 $q(x_1)$ 中的数据样本，但无法访问其密度函数本身。此外，我们令 $p_t$ 为一条概率路径，其中 $p_0 = p$ 为简单分布，例如标准正态分布 $p(x) = \mathcal N(x|0, I)$，并令 $p_1$ 的分布与 $q$ 近似相等。稍后我们将讨论如何构建这样的路径。Flow Matching 目标旨在匹配该目标概率路径，从而使我们能够从 $p_0$ 流向 $p_1$。

## 3.1 CONSTRUCTING $p_t, u_t$ FROM CONDITIONAL PROBABILITY PATHS AND VECTOR FIELDS