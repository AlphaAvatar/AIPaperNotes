论文链接：https://arxiv.org/pdf/2507.02592

代码链接：https://github.com/Alibaba-NLP/WebAge

# 摘要

超越人类认知局限是 LLM 训练的关键目标。像 DeepResearch 这样的专有 Agent 系统，在极其复杂的信息搜索基准测试（例如 BrowseComp）上展现出了超越人类的能力，这在以前是难以企及的。我们认为，**它们的成功取决于开源模型所缺乏的一种复杂推理模式：在探索浩瀚信息时，系统地降低极端不确定性的能力**。基于这一洞见，我们推出了 **WebSailor**，这是一种旨在培养这一关键能力的完整后训练方法。我们的方法包括通过结构化采样和信息混淆、RFT 冷启动以及高效的 Agent 强化学习训练算法——重复采样策略优化 (Duplicating Sampling Policy Optimizati, DUPO)，来生成新的高不确定性任务。凭借这一集成流程，WebSailor 在复杂的信息搜索任务中的表现显著优于所有开源 Agent，与专有  Agent 的性能相媲美，并缩小了能力差距。

# 1.介绍

信息搜索是人类解决不确定性的基本驱动力，而互联网已经彻底改变了这一驱动力。然而，人类驾驭这片广阔数字世界的能力却受到认知局限的制约：有限的记忆、脆弱的注意力，以及无法同时进行多条探索路径。领先的专有 Agent 系统，例如 Deep Research，表明大语言模型 (LLM) Agent 可以超越这些人类局限。它们在 BrowseComp-en/zh 等复杂的网络基准测试中表现出的超凡性能，源于其复杂的推理能力——无论是内部推理还是工具介导的推理——能够系统地降低不确定性。

然而，将这些高级推理能力注入开源 Agent 仍是一个悬而未决的问题。如图 1 所示，现有的开源 LLM 和 Web Agent 在 BrowseComp-en 数据集上的准确率接近于零。**这种显著的性能差距源于当前的训练范式侧重于我们所谓的 1 级和 2 级任务：即不确定性较低（例如单次搜索）或具有清晰、结构化解决路径（例如标准多跳问答）的问题**。这些数据集无法让模型应对在复杂基准测试中占主导地位的 3 级挑战——这些场景要求在没有预定义解决方案路径的复杂信息环境中进行稳健的组合泛化。因此，模型无法开发出应对这些挑战所需的复杂、多步骤推理能力。

为了引出这些超越人类的推理模式，我们生成了具有高度且难以降低的内在不确定性的训练数据。我们的主要机制是从现实世界网站上随机游走生成的互联知识结构中采样子图。从组合泛化的角度来看，这些子图呈现了已知实体和关系的全新组合，迫使模型对此前未曾见过的组合进行推理，并使其超越简单的启发式方法。这一过程生成了各种各样复杂且难以预先定义的涌现结构，迫使模型开发出可能超越现有人类模式的推理过程。

我们利用精心设计的信息混淆技术进一步提升任务难度，这直接增加了初始模糊性。结构复杂性和信息模糊性的结合，使得任务需要极其复杂的推理。例如，我们生成的一些问题极具挑战性，即使是像 o3 这样强大的专有模型也需要调用多达 40 次工具才能得出解决方案，这凸显了我们极力降低不确定性的重要性。

获得 QA 数据后，一个关键挑战是获得具有完全推理过程的有监督数据。虽然强大的开源大型推理模型 (LRM)（例如 QwQ 和 DeepSeek-R1）可以解决一些复杂的问答系统，但它们原生的推理输出不适合直接微调。这些模型表现出高度程式化和冗长的思维过程，如果模仿，可能会限制受训 Agent 开发自身灵活探索性策略的能力。此外，在需要数十次工具调用的长周期 Web 任务中，其冗长的推理链很快就会淹没上下文窗口，导致性能下降和可读性下降。为了解决这个问题，我们提出了一种新的方法：我们利用这些开源 LRM 生成成功的行动观察轨迹，然后重建推理。通过为每个步骤推断简洁、以行动为导向的思维，我们创建了一个清晰有效的监督信号，该信号能够捕捉解决方案的逻辑，而不会继承与程式化或冗长相关的缺陷。

在训练流程优化方面，尽管近期研究建议跳过 SFT，但我们证明，对于处理此类复杂任务的 Web Agent 来说，适度的拒绝采样微调 (RFT) 冷启动是必不可少的。一方面，此类场景的强化学习 (RL) 奖赏极其稀疏，初始反馈通常接近于零。另一方面，我们的方法并不过度依赖蒸馏；只需 2000 多个高质量示例的极简冷启动即可证明其有效性。**由于多轮推理和大量工具的使用，此类任务的强化学习 (RL) Agent 训练速度极其缓慢。为了解决这个问题，我们提出了 Duplicating Sampling Policy Optimizati (DUPO)，它结合了两种动态采样策略——一种在训练前，一种在训练期间——以提高有效性和效率**。

我们的 WebSailor 模型系列（3B、7B、32B 和 72B）在 BrowseComp-en/zh 上的表现优于所有开源模型和 Agent 方法，并且当与浏览功能结合使用时，也超越了 Grok-3 和 DouBao 等专有 LRM，如图 1 所示。此外，我们发现基于复杂的、不确定性驱动的推理模式的后训练表现出向下兼容性，在 GAIA、XBench-DeepSearch 和 SimpleQA 等更简单的任务上取得了良好的性能。

# 2.Problem Definition

我们采用 ReAct 作为 Agent 的框架。收到问题后， Agent 会执行多次 **Thought-Action-Observation** 的迭代。具体来说，在每次迭代中，LLM 基于现有上下文生成一个 Thought 并执行一个可解析的 Action（工具调用），然后等待环境返回一个 Observation。在 WebTraverseX 中，行动空间包括**生成最终答案**以及两个工具——**搜索**和**访问**，这两个工具分别对应于使用多个查询调用搜索引擎以及通过 URL 访问多个网页以检索其内容。这两个工具的详细信息见附录 A.1。搜索操作返回的观察结果包含 10 个标题、摘要及其与每个搜索查询对应的 URL。相比之下，访问操作的观察结果则是网页的摘要，根据 LLM 操作中指定的“目标”进行定制。当 LLM 选择“最终答案”作为操作时，迭代终止。包含 $T$ 次迭代的完整轨迹可以定义为：

$$\mathcal H_{T}=(\tau_0,a_0,o_0,...,\tau_i,a_i,o_i,...,\tau_T,a_T),\tag{1}$$

其中 $\tau_i, a_i, o_i$ 分别表示第 $i$ 轮的想法、动作和观察。在步骤 $t$，$\tau_t$ 和 $a_t$ 从一个基于所有先前上下文的策略中采样，即 $π(a, \tau|\mathcal H_{t−1})$。

完成多跳问答通常只需要一两轮ReAct，因为每一步的操作都非常清晰，无需太多的战略规划。与之形成鲜明对比的是，BrowseComp 将智 Agent 放置在一个庞大的非结构化信息空间中，其中的解决方案路径并未预先定义。单纯的暴力搜索在计算上是不可行的，可能需要数千次工具调用，这将使任何现代LLM的上下文窗口不堪重负。因此，成功的关键不在于遵循简单的脚本，而在于执行高度自适应的搜索策略。Agent 必须动态地合成部分信息，修剪没有希望的探索路径，并整合不同的事实以最终找到解决方案。将这个组合庞大的搜索空间压缩成几十步的可处理轨迹，需要复杂的思维链。正是这种战略导航和综合的过程，体现了这项工作试图引出和模拟的复杂、超人的推理模式。

# 3.Large-scale Training Data Synthesis for Complex Reasoning

在本节中，我们从两个角度介绍我们的训练数据构建：QA构建和推理轨迹生成。

## 3.1  SailorFog-QA: Scalable Graph-Synthesized QA

回答问题所需的推理模式取决于问题本身的**不确定性**以及降低不确定性的**复杂性**。如图2所示，我们基于这两个维度将信息搜索型问答系统分为三个级别。
- **Level 1**：任务表现出较低的不确定性，并且很容易降低这种不确定性。这些问题可以通过模型的内部知识或通过单一、直接的网络搜索来回答。
- **Level 2**：诸如多跳问答之类的任务，初始不确定性较高，但会遵循清晰的解决路径。即使步骤繁多，实体之间也通过定义明确的逻辑连接，从而可以通过结构化的操作序列系统地降低不确定性。
- **Level 3**：我们工作的重点是解决高度不确定性且其化解难度高的问题。这类问题中，实体以复杂且突发的方式耦合，缺乏预定义的推理路径。解决这些问题需要创造性的探索和难以手动指定的新推理模式。

**Constructing the Structural Foundation for Hard-to-Reduce Uncertainty**。为了生成三级任务，我们首先构建一个复杂的信息图，其中不确定性本质上难以降低。我们的流程受到随机游走的启发，构建了具有涌现性、非线性结构的知识图谱。我们首先从 Wikidata 的 SPARQL 服务中检索一个模糊实体，并将其作为图谱的种子，以确保起点具有挑战性。通过模拟网页浏览，我们从互联网上收集关于该实体的非结构化文本和特征。从这些原始信息中，我们提取相关实体及其连接关系，形成初始节点和边。**关键步骤是迭代扩展**：我们概率性地选择现有节点，并寻找新的、不同的实体进行连接。这个随机过程不鼓励简单的线性链（二级任务的特征），而是促进一个具有复杂且重叠关系路径的紧密互连图谱。生成的图谱为缺乏预定义推理路径的问题提供了结构基础，迫使 Agent 在复杂的信息网络中导航，而不是沿着直线前进。

**Generating High-Uncertainty Questions via Subgraph Sampling and Obfuscation**。以这些复杂的图为基础，我们生成具有高度初始不确定性的问题。这是通过对具有不同拓扑结构的子图进行采样来实现的，每个子图代表一组独特的耦合实体和关系。然后，我们基于子图构建问题和答案。至关重要的是，**我们通过刻意的信息混淆来引入歧义**。我们不是呈现清晰的事实，而是混淆问题中的特征和关系。例如，将精确的日期转换为模糊的时期（“2010 年代初”），将名称部分隐藏（“由首字母为“F”的人创立的机构”），或将定量属性定性描述（“市场份额低于 1%）。这种混淆直接增加了初始不确定性，迫使 Agent 进行推理、比较和综合信息，而不是简单地执行查找。我们将合成的训练数据命名为 **SailorFog-QA**，它具有三大关键优势：
- 这些数据以现实世界的互联网为基础，反映了 Agent 在实践中面临的挑战。
- 多样化的子图拓扑自然会产生需要一系列复杂推理模式的问题，从多步推理到组合和比较分析。
- 该方法具有高度可扩展性，因为潜在子图的数量（以及由此产生的挑战性问题）随着图的大小呈非线性增长，从而实现高效的大规模数据合成。

为了说明我们生成的三级任务的特点，下面给出了两个示例。这些问题体现了我们的方法论：它们包含多个错综复杂的实体，以及刻意混淆的信息，例如模糊的时间指代（“大约5世纪中叶”、“21世纪初”）和不明确的描述（“著名的南美洲首都”、“受人尊敬的艺术机构”）。这种**结构复杂性**和**信息模糊性**的结合造成了高度的初始不确定性，而且这种不确定性极难消除。事实上，我们的人工评估证实，在典型的时间限制（例如两小时内）下，这些问题对于人类研究人员来说是难以解决的，因为它们缺乏明确的搜索起点，并且需要进行大量的非线性探索。有关我们问答生成流程的更多详细信息，请参阅附录A.2。

## 3.2 Reconstructing Reasoning from Expert LRM Trajectories

合成复杂的 QA 对后，下一个挑战是为冷启动有监督生成相应的解决方案轨迹。虽然像 QwQ-32B 这样的强大的开源 LRM 可以提供一些正确的轨迹，但直接使用它们的全部输出进行微调会适得其反。我们发现了两个关键问题：
- **Stylistic Contamination**：这些 LRM 的推理过程往往冗长，且风格化程度较高。直接对这些输出进行微调可能会过于规范，从而抑制Agent 开发自主探索策略和泛化至未知问题的能力。
- **Context Overload**：其推理链的冗长性，对于复杂的 Web Agent任务而言，是一个实际的障碍。包含数十次工具调用的轨迹很容易生成超出上下文限制的历史记录，从而降低性能并使推理过程变得难以处理。

**流程如下**：首先，我们提示一个专家级开源 LRM 生成一条完整的解决方案轨迹，包括其原生思维过程。从这条完整轨迹中，我们有选择地丢弃 LRM 原始的、冗长的思维，只保留成功的 action-observation 序列 $(a_0, o_0, a_1, o_1, ...)$。这条轨迹代表了解决方案路径的“what”和“how”，但不代表“why”。

接下来，我们重构缺失的 “why”。对于行动轨迹中的每一步 $t$，我们具有上一步的历史记录 $\mathcal H_{t−1} = (\hat\tau_0, a_0, o_0, . . , \hat τ_{t−1}, a_{t−1}, o_{t−1})$，以及专家选择的动作 $a_t$ 和后续的观察结果 $o_t$。然后，我们启动一个独立且强大的指令遵循模型 $π^∗$，生成一个新的想法 $\hat\tau_t$，作为采取行动 $a_t$ 的简洁、合乎逻辑的理由：

$$\hat\tau_t\sim\pi^*(\tau|\mathcal H_{t-1},a_t,o_t).\tag{2}$$

通过在每个步骤中迭代应用此方法，我们合成了一条完整、高质量的推理轨迹 $\mathcal {\hat H}_T = (\hat\tau_0, a_0, o_0, . . . , \hat\tau_T, a_T, o_T)$，其中推理清晰且以目标为导向。为了进行重构，我们使用了另一个 LLM 并强制采用“**短 CoT**”风格。这是一个关键的设计选择，确保最终的推理链足够紧凑，能够胜任长期任务。这种方法使我们能够可扩展地生成有监督数据，从而灌输复杂的推理模式，而不会产生直接模仿的负面影响。

# 4.Reinforcement Learning with Cold Start

我们的训练方法分为两个阶段。受近期后训练领域的进展（强调在进行更复杂的学习之前进行有针对性的微调的有效性）的启发，我们首先采用适度的 RFT 阶段作为“冷启动”。此初始阶段旨在使模型具备基本的工具使用能力，并遵循长推理框架。随后，我们利用强化学习进一步完善 Agent 的推理能力，提高其样本效率，并更充分地利用我们高质量的复杂训练数据。

## 4.1 Rejection Sampling Fine-Tuning

**Setup**。在完整的轨迹 $\mathcal H_T$ 中，Agent 的思维 ($\tau_i$) 用 $\text{<think>}$ 和 $\text{</think>}$ 标签包裹。动作 ($a_i$) 用 $\text{<tool_call>}$ 和 $\text{</tool_call>}$ 来区分函数调用，或用 $\text{<answer>}$ 和 $\text{</answer>}$ 来区分最终响应。工具调用产生的环境观测值 ($o_i$) 用 $\text{<tool_response>}$ 和 $\text{</tool_response>}$ 标签包裹。不同的片段由这些特殊 token 分隔。

**Filtering**。我们对专家生成的轨迹应用了三阶段过滤流程。首先，为了保证有监督信号的正确性，我们进行拒绝采样，只保留最终得出正确答案的轨迹。其次，鉴于专家模型比我们的策略模型拥有更出色的长上下文处理能力，我们丢弃了所有长度超过 32k 个 token 的轨迹。第三，我们根据任务复杂性进行过滤，保留那些工具调用次数超过 5 次的轨迹，因为复杂的推理模式和有效的规划策略通常体现在更长的决策步骤序列中。

**Training objective**。训练的目标是专门增强 Agent 的决策能力，即其产生有效想法和行动的能力。因此，与环境观测值 ($o_i$) 对应的 token 会被屏蔽在损失计算之外。

## 4.2 Duplicating Sampling Policy Optimization

在 RFT 冷启动阶段之后，该阶段为模型配备了基本的工具使用能力并遵循推理框架，我们提出了重复采样策略优化（DUPO）来进一步完善推理能力，**提高样本效率**并最终激发其内在潜力，发现和内化超越直接模仿的复杂问题解决策略。

Agent 强化学习与传统推理任务的强化学习的主要区别在于，Agent 是一个涉及与环境交互（工具响应）的多轮过程。然而，与环境的交互导致 Agent 强化学习的推理速度远低于标准强化学习。**DAPO 采用动态采样来过滤掉完全正确或错误的结果，然后用新的 QA 将批次填充到目标大小**。虽然这对于数据管理很有效，但它可能需要在同一批次中对不同案例进行顺序推理。这种顺序处理进一步加剧了 Agent 强化学习训练速度缓慢的固有特性。

为了解决这个问题，我们首先在训练前过滤掉过于简单的案例（8 个 rollout 全部正确的案例）。**训练期间，我们不再使用填充来扩展批次，而是从同一批次中复制具有非零标准差的样本**。与 DAPO 的动态采样相比，这种方法实现了大约 2-3 倍的加速。与 SFT 类似，在计算策略损失时也需要屏蔽观测值。我们遵循 GPRO 的方法，以组相关方式估计优势。我们还利用了 DAPO 中的 token 级策略梯度损失和更高的裁剪技术。DUPO 的训练目标定义如下：

J(θ) 的目标函数定义为：

$$
J(\theta) = \mathbb{E}_{(q,y)\sim \mathcal{D},\{o_i\}_{i=1}^G\sim \pi_{\theta_{\mathrm{old}}}(\cdot\mid \mathrm{context})}
\left[
\frac{1}{\sum_{i=1}^G |o_i|}\;\sum_{i=1}^G\sum_{t=1}^{|o_i|}
\min\Bigl(r_{i,t}(\theta)\,\hat A_{i,t},\;\mathrm{clip}\bigl(r_{i,t}(\theta),\,1-\epsilon_{\mathrm{low}},\,1+\epsilon_{\mathrm{high}}\bigr)\,\hat A_{i,t}\Bigr)
\right]
\tag{3}$$

且需满足约束条件

$$
0 < \bigl|\{\,o_i \mid \mathrm{is\_equivalent}(y,o_i)\}\bigr| < G,
$$

其中，$(q,y)$ 是问答对，$r_{i,t}(\theta)$ 是重要性采样比率，$\hat A_{i,t}$ 是第 $t$ 步的优势估计：

$$
r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t}\mid \mathrm{context})}{\pi_{\theta_{\mathrm{old}}}(o_{i,t}\mid \mathrm{context})},
\qquad
\hat A_{i,t} = \frac{R_i - \mathrm{mean}(\{R_i\}_{i=1}^G)}{\mathrm{std}(\{R_i\}_{i=1}^G)}.
\tag{4}$$

注意，在公式 (4) 中，$o_i$ 仅表示模型生成的 token 序列，而非完整轨迹；`context` 包括模型生成过程及工具响应。对于标准差为 0 的情况（即所有 rollout 回答要么完全正确、要么完全错误），会将该批次对应槽位移除，并用同批次中标准差不为 0 的其他样本随机复制填补。

为避免奖励“投机”行为，我们采用结合格式验证和答案验证的规则化奖励：

$$
R_i = 0.1 \times R_i^{\mathrm{format}} \;+\; 0.9 \times R_i^{\mathrm{answer}}.\tag{5}
$$

其中，格式得分用于检查 rollout 轨迹是否符合预定义格式（如不同内容片段是否正确包裹在 `<think>` 与 `<tool_call>` 标签中，序列是否遵循 ReAct 框架）；答案得分则由 LLM 作为裁判，判定最终预测是否正确。

# 5.Experiments
