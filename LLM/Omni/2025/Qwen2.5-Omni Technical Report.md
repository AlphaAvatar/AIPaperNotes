论文链接：https://arxiv.org/pdf/2503.20215

代码链接：

# 摘要
在本报告中，我们介绍了 Qwen2.5-Omni，这是一个端到端的多模态模型，**旨在感知多种模态，包括文本、图像、音频和视频**，同时以流式传输的方式生成文本和自然语音响应。为了实现多模态信息输入的流式传输，音频和视觉编码器均采用了分块处理方法。该策略有效地解耦了长序列多模态数据的处理，**将感知任务分配给多模态编码器，并将扩展序列的建模委托给大语言模型**。这种分工通过共享注意力机制增强了不同模态的融合。**为了同步视频输入和音频的时间戳，我们以交错的方式按顺序组织音频和视频**，并提出了一种新的位置嵌入方法，称为 **TMRoPE**（时间对齐多模态 RoPE）。为了同时生成文本和语音，同时避免两种模态之间的干扰，我们提出了 **Thinker-Talker** 架构。在这个框架中，Thinker 是一个大语言模型，负责文本生成；而 Talker 是一个双轨自回归模型，它直接利用 Thinker 的隐藏表征生成音频 token 作为输出。Thinker 和 Talker 模型均设计为端到端训练和推理。为了以流式方式解码音频 token，我们引入了一个滑动窗口 DiT 来限制感受野，旨在减少初始数据包延迟。Qwen2.5-Omni 与类似大小的 Qwen2.5-VL 相当，并且优于 Qwen2-Audio。此外，Qwen2.5-Omni 在 Omni-Bench 等多模态基准测试中达到了最佳性能。值得注意的是，Qwen2.5-Omni 在端到端语音指令遵循方面的表现与其在文本输入方面的能力相当，MMLU 和 GSM8K 等基准测试就是明证。在语音生成方面，Qwen2.5-Omni 的流式 Talker 在稳健性和自然性方面优于大多数现有的流式和非流式替代方案。

# 1.介绍
在日常生活中，人类能够同时感知周围的视觉和听觉信息。经过大脑处理后，人类会通过书写、发声或使用工具（以及肢体动作）进行反馈，从而与世界上各种生物进行信息交换，展现出智能。近年来，通用人工智能日益受到人们的关注，这主要得益于大语言模型 (LLM) 的进步。这些模型基于海量文本数据进行训练，代表了人类创造的高级离散表征，展现了解决复杂问题和快速学习的能力。此外，在理解领域，语言-听觉-语言模型 (LALM) 和语言-视觉-语言模型 (LVLM) 帮助 LLM 以端到端的方式进一步扩展了听觉和视觉能力。然而，如何高效地以端到端的方式统一所有这些不同的理解模式，充分利用数据，并以类似于人类交流的文本和语音流提供响应，仍然是一项重大挑战。

开发统一智能的全模态模型需要仔细考虑几个关键因素。首先，必须实现一种系统的方法，对文本、图像、视频和音频等各种模态进行联合训练，以促进它们之间的相互增强。**这种协调对于视频内容尤为重要，因为视频内容需要同步音频和视频信号的时间方面**。其次，必须管理不同模态输出之间的潜在干扰，确保文本和语音 token 等输出的训练过程不会相互干扰。最后，需要探索能够实时理解多模态信息并实现高效音频输出流的架构设计，从而减少初始延迟。

在本报告中，我们介绍了 Qwen2.5-Omni，这是一个统一的单模型，能够处理多种模态，并以流格式同时生成文本和自然语音响应。为了应对第一个挑战，我们提出了一种新的位置嵌入方法，称为 **TMRoPE**（**T**ime-aligned **M**ultimodal **RoPE**）。我们将这些音频和视频帧组织成交错结构，以按时间顺序表示视频序列。对于第二个挑战，我们提出了 Thinker-Talker 架构，其中 Thinker 负责文本生成，而 Talker 专注于生成流式语音 token。Talker 直接从 Thinker 接收高级表示。这种设计的灵感来自于人类利用不同器官产生各种信号的方式，这些信号同时通过相同的神经网络进行协调。因此，Thinker-Talker 架构是端到端联合训练的，每个组件专用于生成不同的信号。为了应对流式传输带来的挑战，并促进实时理解多模态信号所需的预填充，我们建议对所有多模态编码器进行修改，采用分块流式处理方法。为了支持流式语音生成，我们实现了一个双轨自回归模型来生成语音 token，以及一个DiT模型来将这些 token 转换为波形，从而实现流式音频生成并最大限度地减少初始延迟。此设计旨在使模型能够实时处理多模态信息并有效地进行预填充，从而实现文本和语音信号的并发生成。

Qwen2.5-Omni 与规模相近的 Qwen2.5-VL 性能相当，并在图像和音频性能方面优于 Qwen2-Audio。此外，Qwen2.5-Omni 在 OmniBench 和 AV-Odyssey Bench 等多模态基准测试中取得了最佳性能。值得注意的是，Qwen2.5-Omni 在端到端语音指令遵循方面的表现与其在文本输入方面的表现相当，这在 MMLU 和 GSM8K 等基准测试中得到了证实。在语音生成方面，Qwen2.5-Omni 在 seed-tts-eval test-zh、test-en 和 test-hard 集上分别实现了 1.42%、2.33% 和 6.54% 的字错误率 (WER)，优于 MaskGCT 和 CosyVoice 2。

Qwen2.5-Omni 的主要特点可以概括为：
- 我们推出了 Qwen2.5-Omni，这是一个统一的模型，可以感知所有模态并以流式方式同时生成文本和自然语音响应。
- 我们提出了一种新的位置嵌入算法，称为 TMRoPE，它明确地结合了时间信息来同步音频和视频。
- 我们提出了 架构来促进实Thinker-Talker 时理解和语音生成。
- 与规模相近的单模态模型相比，Qwen2.5-Omni 在所有模态上均展现出强劲的性能。它显著提升了语音指令的执行能力，达到了与纯文本输入相当的性能水平。对于涉及多模态整合的任务，例如在 OmniBench 中评估的任务，Qwen2.5-Omni 达到了最佳性能。值得注意的是，Qwen2.5-Omni 在 seed-tts-eval 上表现出色，展现出强大的语音生成能力。

# 2.Architecture
## 2.1 Overview
## 2.2 Perceivation
## 2.3 Generation
## 2.4  Designs for Streaming
# 3. Pre-training