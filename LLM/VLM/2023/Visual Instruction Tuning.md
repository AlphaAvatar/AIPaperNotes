论文链接：https://arxiv.org/pdf/2304.08485

代码链接：https://llava-vl.github.io

# 摘要

使用机器生成的指令遵循数据对大语言模型 (LLM) 进行指令微调已被证明能够提升其在新任务上的零样本能力，但在多模态领域，这一思路的探索较少。**我们首次尝试使用纯语言的 GPT-4 生成多模态语言图像指令遵循数据**。通过对此类生成的数据进行指令微调，我们推出了  **LLaVA**: **L**arge **L**anguage **a**nd **V**ision **A**ssistant，这是一个端到端训练的大型多模态模型，它连接视觉编码器和 LLM，用于通用的视觉和语言理解。为了促进未来对视觉指令遵循的研究，我们构建了两个面向应用的评估基准，其中包含各种具有挑战性的任务。我们的实验表明，LLaVA 展现出令人印象深刻的多模态聊天能力，有时甚至能表现出多模态 GPT-4 在未见过的图像/指令上的行为，并且在合成的多模态指令遵循数据集上，其得分与 GPT-4 相比高出 85.1%。在 Science QA 上进行微调后，LLaVA 与 GPT-4 的协同效应达到了 92.53% 的最高准确率。我们已将 GPT-4 生成的视觉指令微调数据、我们的模型和代码公开。

# 1.介绍

人类通过视觉和语言等多种渠道与世界互动，因为每种渠道在表达和传达特定概念方面都有其独特的优势，从而有助于更好地理解世界。人工智能的核心愿景之一是开发一种通用助手，能够有效地遵循多模态视觉和语言指令，并与人类意图保持一致，从而完成各种现实世界的任务。

为此，社区见证了开发**语言增强的基础视觉模型**的发展，这种模型在开放世界视觉理解方面具有强大的能力，例如分类、检测、分割和字幕生成，以及视觉生成和编辑。我们推荐读者参阅《Computer Vision in the Wild》阅读清单，以获取最新的文献汇编。在这一系列的工作中，每个任务都由一个大型视觉模型独立解决，任务指令在模型设计中隐式考虑。此外，语言仅用于描述图像内容。虽然这使得语言能够在将视觉信号映射到语言语义（人类交流的常用渠道）中发挥重要作用，但它导致模型通常具有固定界面，交互性和对用户指令的适应性有限。

另一方面，大语言模型 (LLM) 已表明语言可以发挥更广泛的作用：它可以作为通用助手的通用接口，各种任务指令可以用语言明确表示，并引导端到端训练的神经助手切换到感兴趣的任务并进行解决。例如，ChatGPT 和 GPT-4 近期的成功证明了对齐 LLM 在遵循人类指令方面的强大能力，并激发了人们对开发开源 LLM 的极大兴趣。其中，LLaMA 是一个开源模型，它利用各种机器生成的高质量指令遵循样本来提升 LLM 的对齐能力，与专有 LLM 相比，其性能令人印象深刻。重要的是，这项工作目前仅基于文本。

在本文中，我们提出了视觉指令微调技术，这是首次尝试将指令微调技术扩展到语言-图像多模态空间，旨在为构建通用的视觉助手铺平道路。具体而言，我们的论文做出了以下贡献：
- **Multimodal instruction-following data**。一个关键挑战是缺乏视觉语言指令遵循数据。我们提出了一种数据重组视角和流程，利用 ChatGPT/GPT-4 将图像文本对转换为合适的指令遵循格式。
- **Large multimodal models**。我们开发了一个大型多模态模型 (LMM)，通过将 CLIP 的视觉编码器与 Vicuna 的语言解码器连接起来，并在我们生成的视觉-语言指令数据上进行端到端微调。我们的实证研究验证了使用生成数据进行 LMM 指令调优的有效性，并提出了构建通用指令遵循视觉 Agent 的实用技巧。当与 GPT-4 集成时，我们的方法在 Science QA 多模态推理数据集上实现了 SoTA。
- **Multimodal instruction-following benchmark**。我们向 LLaVA-Bench 提供了两个具有挑战性的基准，其中包含多种配对图像、说明和详细注释。
- **Open-source**。我们向公众发布以下资产：生成的多模式指令数据、代码库、模型检查点和可视化聊天演示。

# 2.相关工作

**Multimodal Instruction-following Agents**。在计算机视觉领域，构建指令遵循 Agent 的现有研究大致可分为两类：(i) 端到端训练模型，针对每个特定的研究主题分别进行探索。例如，视觉语言导航任务和 Habitat 要求具身人工智能 Agent 遵循自然语言指令，并采取一系列动作来完成视觉环境中的目标。在图像编辑领域，给定输入图像和指示 Agent 执行操作的书面指令，InstructPix2Pix 会按照人类指令编辑图像。(ii) 通过 LangChain / LLM 协调各种模型的系统，例如 Visual ChatGPT、X-GPT、MM-REACT、VisProg 和 ViperGPT。在构建指令遵循 Agent 方面有着共同目标的同时，我们专注于开发用于多种任务的端到端训练的语言视觉多模态模型。

**Instruction Tuning**。在自然语言处理 (NLP) 领域，为了使 GPT-3、T5、PaLM 和 OPT 等 LLM 能够遵循自然语言指令并完成现实世界的任务，研究人员探索了 LLM 指令微调的方法，并分别衍生出 InstructGPT/ChatGPT、FLAN-T5、FLAN-PaLM 和 OPT-IML 等指令微调模型。事实证明，这种简单的方法可以有效提升 LLM 的零样本和小样本泛化能力。因此，将 NLP 中的这一思想借鉴到计算机视觉领域是很自然的。更广泛地说，基于基础模型的师生蒸馏思想已在图像分类等其他领域得到了研究。 Flamingo 可以被视为多模态领域的 GPT-3 时刻，因为它在零样本任务迁移和上下文学习方面表现出色。其他基于图文对训练的语言模型 (LMM) 包括 BLIP-2、FROMAGe 和 KOSMOS-1。PaLM-E 是一个用于具身人工智能的语言模型 (LMM)。OpenFlamingo 和 LLaMA-Adapter 基于近期“最佳”开源语言模型 (LLM) LLaMA，使 LLaMA 能够使用图像输入，为构建开源多模态语言模型 (LLM) 铺平了道路。虽然这些模型展现出良好的任务迁移泛化性能，但它们并未明确针对视觉语言指令数据进行微调，并且它们在多模态任务中的表现通常不如纯语言任务。本文旨在填补这一空白并研究其有效性。最后需要注意的是，**视觉指令微调不同于视觉提示微调：前者旨在提高模型的指令遵循能力，而后者旨在提高模型自适应中的参数效率**。

# 3.GPT-assisted Visual Instruction Data Generation

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0dc42605a35341848cbd7db55232fbdd.png)

社区见证了公开多模态数据（例如图文对）数量的激增，范围从 CC 到 LAION。然而，对于多模态指令遵循数据，可用的数据量却有限，部分原因是创建此类数据的过程耗时，而且考虑到人工众包搜索，其定义也较为模糊。受近期 GPT 模型在文本标注任务中成功的启发，我们建议利用 ChatGPT/GPT-4 进行多模态指令遵循数据收集，该数据基于广泛存在的图像对数据。

对于图像 $\textbf X_v$ 及其相关标题 $\textbf X_c$，创建一组问题 $X_q$ 是很自然的，旨在指导 Assistant 描述图像内容。我们提示 GPT-4 整理这样一份问题列表（详见附录）。因此，将图文对扩展为指令执行版本的一种简单方法是：$Human: \textbf X_q\textbf X_v<STOP> Assistant: \textbf X_c<STOP>$。虽然构建成本低廉，但这种简单的扩展版本在指令和响应方面都缺乏多样性和深度推理。

为了缓解这个问题，我们利用纯语言的 GPT-4 或 ChatGPT 作为强教师模型（两者均仅接受文本输入），来创建涉及视觉内容的指令遵循数据。具体而言，为了将图像编码到其视觉特征中以提示纯文本 GPT，我们使用两种类型的**符号表征**：(i) 标题通常从不同视角描述视觉场景；(ii) 边界框通常用于定位场景中的物体，每个边界框都编码了物体的概念及其空间位置。表 14 的顶部区块展示了一个示例。

这种符号表示使我们能够将图像编码为 LLM 可识别的序列。我们使用 COCO 图像并生成三种类型的指令遵循数据。表 14 底部区块中显示了每种类型的一个示例。对于每种类型，我们首先手动设计一些示例。它们是我们在数据收集过程中仅有的人工注释，并用作上下文学习的种子示例，用于提示 GPT-4。
- **Conversation**。我们设计了一段助手与用户之间的对话，用户会就这张照片提问。回答的语气听起来就像助手正在看照片并回答问题一样。我们会询问一系列关于图片视觉内容的问题，包括物体类型、物体数量、物体动作、物体位置以及物体之间的相对位置。只有答案明确的问题才会被考虑。详细提示请参阅附录。
- **Detailed description**。为了给图像提供丰富全面的描述，我们创建了一个带有此意图的问题列表。我们会提示 GPT-4，然后整理该列表（请参阅附录中详细的提示和整理流程）。对于每张图片，我们会从列表中随机抽取一个问题，让 GPT-4 生成详细的描述。
- **Complex reasoning**。以上两种类型侧重于视觉内容本身，并在此基础上进一步构建深度推理问题。答案通常需要循序渐进的推理过程，遵循严谨的逻辑。

我们总共收集了 158,000 个独特的语言-图像指令遵循样本，其中对话类 58,000 个，详细描述类 23,000 个，复杂推理类 77,000 个。在早期实验中，我们对比了 ChatGPT 和 GPT-4 的使用，发现 GPT-4 始终能够提供更高质量的指令遵循数据，例如空间推理。

# 4.Visual Instruction Tuning

## 4.1 Architecture

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0b58b45d73a94b9a8e6a46a7e2e4ce9b.png)

主要目标是有效利用预训练的LLM和视觉模型的功能。网络架构如图1所示。我们选择 Vicuna 作为我们的 LLM $f_ϕ(·)$，其参数为 $ϕ$，因为它在公开可用的 checkpoint 的中拥有最佳的指令遵循能力。

对于输入图像 $\textbf X_v$，我们考虑使用预训练的 CLIP 视觉编码器 ViT-L/14，它提供视觉特征 $\textbf Z_v = g(\textbf X_v)$。实验中考虑了最后一个 Transformer 层之前和之后的网格特征。我们考虑使用一个简单的线性层将图像特征连接到词嵌入空间。具体来说，我们应用可训练的投影矩阵 $W$ 将 $\textbf Z_v$ 转换为语言嵌入 token $\textbf H_v$，其维度与语言模型中的词嵌入空间相同：

$$\textbf H_v=\textbf W\cdot\textbf Z_v,~with~\textbf Z_v=g(\textbf X_v)\tag{1}$$

因此，我们得到了一个视觉 token 序列 Hv。需要注意的是，我们的简单投影方案是轻量级的，这使我们能够快速迭代以数据为中心的实验。我们还可以考虑更复杂的方案来连接图像和语言表征，例如 Flamingo 中的门控交叉注意力机制和 BLIP-2 中的 Q-former。我们将探索 LLaVA 更有效、更复杂的架构设计，并将其留待未来研究。

## 4.2 Training

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/55db87f19fc04e37bae6a330e2ee61d2.png)

对于每幅图像 $\textbf X_v$，我们生成多轮对话数据 $(\textbf X^1_q, \textbf X^1_a, · · · , \textbf X^T_q, \textbf X^T_a)$，其中 $T$ 是对话总轮数。我们将它们组织成一个序列，将所有答案视为 Assistant 的回复，并将 $X_t$ 在第 $t$ 轮发出的指令表示为：

$$\textbf X^t_{instruct}=\begin{cases}
Randomly~choose~[\textbf X^1_q,\textbf X_v]~or~[\textbf X_v,\textbf X^1_q], & \text{the first turn }t=1\\
\textbf X^t_q, & \text{the remaining turns }t>1
\end{cases}\tag{2}$$

这导致了表 2 中所示的多模态指令遵循序列的统一格式。我们使用其原始的自回归训练目标对预测 token 上的 LLM 执行指令微调。

具体来说，对于长度为 $L$ 的序列，我们通过以下方式计算目标答案 $\textbf X_a$ 的概率：

$$p(\textbf X_q|\textbf X_v,\textbf X_{instruct})=\prod^L_{i=1}p_{\theta}(x_i|\textbf X_v,\textbf X_{instruct,<i},\textbf X_{a,<i}),\tag{3}$$

其中 $θ$ 是可训练参数，$X_{instruct,<i}$ 和 $X_{a,<i}$ 分别是当前预测 token $x_i$ 之前所有回合的指令和答案 token。预测 token 的说明请参见表 2。对于公示 (3) 中的条件语句，我们明确添加了 $\textbf X_v$ 以强调图像是所有答案的基础，并且为了提高可读性，我们省略了 $\textbf X_{system-message}$ 和所有之前的 $<STOP>$。对于 LLaVA 模型训练，我们考虑采用两阶段指令微调过程。

**Stage 1: Pre-training for Feature Alignment**。为了在概念覆盖率和训练效率之间取得平衡，我们将 CC3M 过滤为 595K 个图文对。有关过滤过程的详细信息，请参阅附录。使用第 3 节中描述的朴素扩展方法将这些图像文本对转换为指令遵循数据。每个样本可以视为单轮对话。为了构建 (2) 中的输入 $\textbf X_{instruct}$，对于图像 $\textbf X_v$，随机采样一个问题 $\textbf X_q$，这是一个语言指令，要求assistant 简要描述图像。真实预测答案 $X_a$ 是原始标题。在训练中，我们保持视觉编码器和 LLM 权重不变，并且仅使用可训练参数 $θ = W$（投影矩阵）最大化 (3) 的似然值。通过这种方式，图像特征 Hv 可以与预训练的 LLM 词嵌入对齐。此阶段可以理解为为冻结的 LLM 训练兼容的视觉标记器。

**Stage 2: Fine-tuning End-to-End**。我们始终保持视觉编码器权重不变，并持续更新 LLaVA 中投影层和 LLM 的预训练权重；即，可训练参数为 (3) 中的 $θ = {W, ϕ}$。我们考虑两种具体的用例场景：
- *Multimodal Chatbot*。我们通过对第三部分中的158K语言-图像指令遵循数据进行微调，开发了一个聊天机器人。三种类型的回复中，对话是多轮的，而其他两种是单轮的。它们在训练中被均匀采样。
- *Science QA*。我们在 ScienceQA 基准数据集上研究了我们的方法，该数据集是第一个大规模多模态科学问题数据集，它用详细的讲解和解释来标注答案。每个问题都以自然语言或图像的形式提供上下文。Assistant 以自然语言提供推理过程，并从多个选项中选择答案。在 (2) 中的训练中，我们将数据组织为单轮对话，问题和上下文组成 $X_{instruct}$，推理和答案组成 $X_a$。

# 5.实验
