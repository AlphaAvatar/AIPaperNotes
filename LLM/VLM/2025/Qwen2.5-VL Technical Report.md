论文链接：https://arxiv.org/pdf/2502.13923

代码链接：https://github.com/QwenLM/Qwen2.5-VL

# 摘要

我们隆重推出 Qwen 视觉语言系列的最新旗舰型号 Qwen2.5-VL，它在基础功能和创新特性方面均实现了显著提升。Qwen2.5-VL 通过增强的视觉识别、精准的物体定位、强大的文档解析和长视频理解能力，在理解和与世界互动方面取得了重大突破。Qwen2.5-VL 的一个突出特点是能够使用边界框或点精确定位物体。它能够从发票、表单和表格中提取强大的结构化数据，并对图表、示意图和布局进行详细分析。为了处理复杂的输入，Qwen2.5-VL 引入了动态分辨率处理和绝对时间编码，使其能够处理不同尺寸的图像和长达数小时的视频，并实现二级事件定位。这使得该模型能够原生感知空间尺度和时间动态，而无需依赖传统的归一化技术。通过从零开始训练原生动态分辨率视觉 Transformer (ViT) 并结合窗口注意力机制，我们在保持原生分辨率的同时显著降低了计算开销。因此，Qwen2.5-VL 不仅在静态图像和文档理解方面表现出色，而且作为一个交互式视觉 Agent，能够在计算机和移动设备等真实场景中进行推理、工具使用和任务执行。该模型无需针对特定任务进行微调即可实现跨领域的强大泛化能力。Qwen2.5-VL 提供三种尺寸，可满足从边缘 AI 到高性能计算的各种应用场景。旗舰级 Qwen2.5-VL-72B 模型可与 GPT-4o 和 Claude 3.5 Sonnet 等最先进的模型相媲美，尤其在文档和图表理解方面表现卓越。较小的 Qwen2.5-VL-7B 和 Qwen2.5-VL-3B 模型性能优于同类竞争对手，即使在资源受限的环境中也能提供强大的功能。此外，Qwen2.5-VL 保持了强大的语言性能，保留了 Qwen2.5 LLM 的核心语言能力。

# 1.Introduction

大型视觉语言模型（LVLM）代表了人工智能领域的一项关键突破，标志着多模态理解和交互方式的变革。通过将视觉感知与自然语言处理无缝集成，这些先进的模型正在从根本上重塑机器解释和分析跨领域复杂信息的方式。尽管多模态大语言模型取得了显著进展，但这些模型目前的水平可以比作夹心饼干的中间层——能够胜任各种任务，但性能却不尽如人意。细粒度的视觉任务构成了这一比喻的基础层。**在本次 Qwen2.5-VL 的迭代中，我们致力于探索细粒度的感知能力**，旨在为 LVLM 构建一个强大的基础，并为实际应用创建一个智能放大器。该框架的顶层是多模态推理，它通过利用最新的 Qwen2.5 LLM 和采用多模态 QA 数据构建来增强。

一系列研究成果促进了多模态大模型的发展，这些模型以架构设计、视觉输入处理和数据管理为特征。LVLM 发展的主要驱动力之一是架构的持续创新。(Alayrac et al., 2022; Li et al., 2022a; 2023b; Liu et al., 2023b;a; Wang et al., 2024i; Zhang et al., 2024b; Wang et al., 2023) 等研究逐步构建了当前的范式，该范式通常由视觉编码器、跨模态投影层和 LLM 组成。细粒度感知模型已成为另一个关键领域。诸如（Xiao et al., 2023; Liu et al., 2023c; Ren et al., 2024; Zhang et al., 2024a;d; Peng et al., 2023; Deitke et al., 2024）等模型拓展了精细视觉理解的边界。Omni 和 MoE 的架构也启发了未来 LVLM 的发展。视觉编码器的改进和分辨率的提升在提高实际视觉理解的质量方面发挥了关键作用。收集更多样化场景和更高质量的数据是训练高级 LVLM 的必要步骤。 (Guo et al., 2024; Chen et al., 2024d; Liu et al., 2024a; Chen et al., 2024a; Tong et al., 2024; Li et al., 2024a) 中提出的努力对这项工作做出了非常有价值的贡献。

然而，尽管视觉语言模型取得了显著进展，但目前仍面临发展瓶颈，包括计算复杂性、有限的上下文理解、较差的细粒度视觉感知以及在不同序列长度上表现不一致等问题。

本报告介绍了我们最新的成果 Qwen2.5-VL，它延续了 Qwen 系列的开源理念，并在多个基准测试中达到甚至超越了顶尖的闭源模型。我们的技术贡献主要体现在以下四个方面：(1) 我们在视觉编码器中实现了窗口注意力机制，以优化推理效率；(2) 我们引入了动态帧率采样，将动态分辨率扩展到时间维度，从而实现了对不同采样率下视频的全面理解；(3) 我们通过与绝对时间对齐，在时间域上改进了 MRoPE，从而促进了更复杂的时间序列学习；(4) 我们投入大量精力整理高质量的数据，用于预训练和有监督微调，并将预训练语料库从 1.2 万亿个 token 扩展到 4.1 万亿个 token。

Qwen2.5-VL 的特性如下：
- **Powerful document parsing capabilities**。Qwen2.5-VL 将文本识别升级为全文档解析，擅长处理多场景、多语言和各种内置（手写、表格、图表、化学式和乐谱）文档。
-  **Precise object grounding across formats**。Qwen2.5-VL 提高了检测、指向和计数物体的精度，支持绝对坐标和 JSON 格式，以实现高级空间推理。
- **Ultra-long video understanding and fine-grained video grounding**。我们的模型将原生动态分辨率扩展到时间维度，增强了理解持续数小时的视频的能力，同时还能在几秒钟内提取事件片段。
- **Enhanced agent Functionality for computer and mobile devices**。利用先进的定位、推理和决策能力，增强智能手机和计算机上的高级 Agent 功能，从而提升模型性能。

# 2.Approach

在本节中，我们首先概述 Qwen2.5-VL 系列模型的架构更新，并提供数据和训练细节的概述。

## 2.1 Model Architecture

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7b9153f4f4504350a8a362fa7135f846.png)

Qwen2.5-VL 的整体模型架构由三个组件构成：

**Large Language Model**。Qwen2.5-VL 系列采用大语言模型作为其基础组件。该模型使用来自 Qwen2.5 LLM 的**预训练权重进行初始化**。为了更好地满足多模态理解的需求，我们将一维旋转位置嵌入 (1D RoPE) 改进为我们提出的与绝对时间对齐的多模态旋转位置嵌入 (Multimodal Rotary Position Embedding to Absolute Time)。

**Vision Encoder**。Qwen2.5-VL 的视觉编码器采用了一种重新设计的视觉 Transformer (ViT) 架构。在结构上，我们融合了 2D-RoPE 和窗口注意力机制，以支持原生输入分辨率，同时加速整个视觉编码器的计算。在训练和推理过程中，输入图像的高度和宽度在输入 ViT 之前都会被调整为 28 的倍数。视觉编码器通过将图像分割成步长为 14 的图像块来处理图像，从而生成一组图像特征。我们将在 2.1.1 节中对视觉编码器进行更详细的介绍。

**MLP-based Vision-Language Merger**。为了解决长序列图像特征带来的效率挑战，我们采用了一种简单而有效的方法，在将特征序列输入大语言模型（LLM）之前对其进行压缩。具体来说，我们并非直接使用视觉 Transformer（ViT）提取的原始图像块特征，而是首先将空间上相邻的四块特征分组。然后，我们将这些分组后的特征连接起来，并通过一个两层的多层感知器（MLP）将其投影到与LLM中使用的文本嵌入相匹配的维度上。这种方法不仅降低了计算成本，而且还提供了一种灵活的方式来动态压缩不同长度的图像特征序列。

表 1 详细列出了 Qwen2.5-VL 的架构和配置。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e362cc13492847dfbaf10ee3052df292.png)

### 2.1.1 Fast and Efficient Vision Encoder

视觉编码器在多模态大语言模型（MLLM）中扮演着至关重要的角色。为了解决由于输入图像分辨率差异而导致的训练和推理过程中计算负载不均衡问题，我们重新设计了视觉Transformer（ViT）架构。一个关键问题是处理不同尺寸图像时计算复杂度呈二次方增长。为了缓解这一问题，我们在大多数层中引入了窗口注意力机制，从而确保计算成本与图像块数量呈线性关系，而非二次方关系。在我们的架构中，只有四层采用了完全自注意力机制，其余层则使用窗口注意力机制，最大窗口大小为 112×112（对应于 8×8 的图像块）。小于112×112的区域不进行填充，保留其原始分辨率。这种设计使得模型能够以输入分辨率原生运行，避免不必要的缩放或失真。

对于位置编码，我们采用二维旋转位置嵌入（RoPE）来有效捕捉二维空间中的空间关系。此外，为了更好地处理视频输入，我们将方法扩展到三维图像块分割。具体来说，我们使用 14×14 的图像块作为基本单元，这与传统的静态图像 ViT 一致。对于视频数据，我们将两个连续帧组合在一起，显著减少了输入到语言模型中的 token 数量。这种设计不仅保持了与现有架构的兼容性，而且提高了处理序列视频数据的效率。

为了简化整体网络结构，我们使 ViT 架构更贴近大语言模型 (LLM) 的设计原则。具体而言，我们采用 RMSNorm 进行归一化，并使用 SwiGLU 作为激活函数。这些选择既提高了计算效率，又增强了模型视觉组件和语言组件之间的兼容性。

在训练方面，**我们从零开始训练重新设计的 ViT 模型**。训练过程包含多个阶段，包括 CLIP 预训练、视觉语言对齐和端到端微调。为了确保模型在不同输入分辨率下的鲁棒性，我们在训练过程中采用原生分辨率的动态采样。图像根据其原始宽高比进行随机采样，使模型能够有效地泛化到不同分辨率的输入。这种方法不仅提高了模型的适应性，而且确保了在不同尺寸的视觉数据上进行稳定高效的训练。

### 2.1.2 Native Dynamic Resolution and Frame Rate

Qwen2.5-VL 在空间和时间维度上都进行了改进，以有效处理各种多模态输入。

在空间域中，Qwen2.5-VL 能够动态地将不同尺寸的图像转换为具有相应长度的 token 序列。与传统的坐标归一化方法不同，我们的模型直接使用输入图像的实际尺寸来表示边界框、点和其他空间特征。这使得模型能够自然地学习尺度信息，从而提高其处理不同分辨率图像的能力。

对于视频输入，Qwen2.5-VL 融合了动态帧率 (FPS) 训练和绝对时间编码。通过适应不同的帧率，该模型能够更好地捕捉视频内容的时间动态。与其他采用文本时间戳或利用额外时间头来实现时间定位的方法不同，我们提出了一种新高效的策略，将 MRoPE ID 直接与时间戳对齐。这种方法使模型能够通过时间维度 ID 之间的间隔来理解时间节奏，而无需任何额外的计算开销。

### 2.1.3 Multimodal Rotary Position Embedding Aligned to Absolute Time

位置嵌入对于视觉和语言模态中的序列数据建模至关重要。基于 Qwen2-VL 中引入的多模态旋转位置嵌入 (MRoPE)，我们扩展了其功能，使其能够更好地处理视频中的时间信息。

**Qwen2-VL 中的 MRoPE 将位置嵌入分解为三个不同的分量：时间、高度和宽度**，从而有效地对多模态输入进行建模。对于文本输入，所有三个分量都使用相同的位置 ID，这使得 MRoPE 在功能上等同于传统的 1D RoPE。对于图像，时间 ID 在所有视觉 token 中保持不变，而高度和宽度分量则根据每个 token 在图像中的空间位置分配唯一的 ID。在处理视频时（视频被视为帧序列），时间 ID 随帧递增，而高度和宽度分量的分配模式与静态图像相同。

然而，在 Qwen2-VL 中，MRoPE 中的时间位置 ID 与输入帧数绑定，这并未考虑内容变化的速度或视频内事件的绝对时间。为了解决这一局限性，**Qwen2.5-VL 引入了一项关键改进：将 MRoPE 的时间分量与绝对时间对齐**。如图 1 所示，通过利用时间 ID 之间的间隔，该模型能够学习不同帧率采样率视频之间的一致时间对齐。

## 2.2 Pre-Training

在本节中，我们首先描述预训练数据集的构建，然后概述整个训练流程和配置。

### 2.2.1 Pre-Training Data

与 Qwen2-VL 相比，我们显著扩展了预训练数据集的规模，从 1.2 万亿个 token 增加到约 4 万亿个 token。我们的预训练数据集是通过多种方法构建的，包括清洗原始网络数据、合成数据等。该数据集涵盖了多种多模态数据，例如图像描述、交错图文数据、光学字符识别 (OCR) 数据、视觉知识（例如名人、地标、动植物识别）、多模态学术问题、定位数据、文档解析数据、视频描述、视频定位以及基于 Agent 的交互数据。在整个训练过程中，我们根据不同阶段的实际情况，精心调整了这些数据类型的组成和比例，以优化学习效果。

**Interleaved Image-Text Data**。图像-文本交错数据对于多模态学习至关重要，它具有三大优势：(1) 支持同时提供视觉和文本线索的上下文学习；(2) 在缺少图像的情况下仍能保持强大的纯文本能力；(3) 包含广泛的通用信息。然而，目前大多数交错数据缺乏有意义的图像-文本关联，且通常存在噪声，这限制了其在复杂推理和创造性生成方面的应用。

为了应对这些挑战，我们开发了一套数据评分和清洗流程，确保仅使用高质量、相关的交错数据。我们的流程包含两个步骤：首先进行标准数据清洗，然后使用内部评估模型进行四阶段评分。评分标准包括：（1）纯文本质量，（2）图像-文本相关性，（3）图像-文本互补性，以及（4）信息密度平衡。这种严谨的方法提高了模型执行复杂推理和生成连贯多模态内容的能力。

以下是对这些图像-文本评分标准的描述：

*图文相关性*：得分越高，表示图像与文本之间的联系越紧密，图像能够有效地补充、解释或扩展文本内容，而不仅仅是起到装饰作用。

*信息互补性*：得分越高，表明图像和文本之间的信息互补性越强。两者都应提供独特的细节，共同构成一个完整的叙事。

*信息密度平衡*：得分越高，表示图像和文本之间的信息分布越均衡，避免文本或图像信息过多，确保两者之间达到适当的平衡。

**Grounding Data with Absolute Position Coordinates**。我们采用原生分辨率训练，旨在更准确地感知世界。相比之下，相对坐标无法有效地表示图像中物体的原始大小和位置。为了克服这一局限性，Qwen2.5-VL 在训练过程中使用基于**输入图像实际尺寸的坐标值**来表示边界框和点。这种方法确保模型能够更好地捕捉物体在现实世界中的尺度和空间关系，从而提升目标检测和定位等任务的性能。

为了提高定位能力的泛化能力，我们开发了一个包含边界框和带有指称表达式的点的综合数据集，该数据集利用了公开数据集和专有数据。我们的方法包括将数据合成为多种格式，包括 XML、JSON 和自定义格式，并采用复制粘贴增强以及与现成模型（例如 Grounding DINO 和 SAM）的合成等技术。这种方法有助于更稳健地评估和改进接地能力。

为了提升模型在开放词汇检测方面的性能，我们将训练数据集扩展到包含超过10000个对象类别。此外，为了提高模型在极端对象检测场景下的有效性，我们在查询中合成了不存在的对象类别，并构建了包含每个对象多个实例的图像数据。

为了确保卓越的基于点的物体定位能力，我们构建了一个包含公开数据和合成数据的综合指向数据集。具体而言，该数据源包括来自 PixMo的公开指向和计数数据、公开的物体定位数据（来自物体检测和实例分割任务），以及通过自动化流程合成的指向特定图像细节的精确指向数据。

**Document Omni-Parsing Data**。为了训练 Qwen2.5-VL 模型，我们合成了一个大型文档语料库。传统的文档内容解析方法通常依赖于独立的模型来处理布局分析、文本提取、图表解释和插图处理。相比之下，Qwen2.5-VL 旨在赋予通用模型全面的文档解析、理解和转换能力。具体来说，我们在文档中融入了多种元素，例如表格、图表、公式、自然或合成图像、乐谱和化学式。这些元素均采用统一的 HTML 格式，HTML 将布局框信息和插图描述整合到 HTML 标签结构中。我们还根据典型的阅读顺序丰富了文档布局，并将每个模块（例如段落和图表）对应的坐标包含在基于 HTML 的真实标签中。这种创新方法使得任何文档的完整信息，包括其布局、文本、图表和插图，都能以标准化和统一的方式呈现。因此，Qwen2.5-VL 实现了多模态文档元素的无缝集成，从而促进了更高效、更准确的文档理解和转换。

以下是 QwenVL 的 HTML 格式：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5a797008582f43f9997e7a3d07b5681f.png)

此格式确保所有文档元素以结构化和易于访问的方式表示，从而使 Qwen2.5-VL 能够高效地处理和理解。

**OCR Data**。我们收集并整理了来自不同来源的数据，以提升OCR性能，包括合成数据、开源数据和内部收集的数据。合成数据通过可视化文本生成引擎生成，以生成高质量的自然场景文本图像。为了支持更广泛的语言并增强多语言能力，我们整合了一个大规模的多语言OCR数据集。该数据集支持多种语言，例如法语、德语、意大利语、西班牙语、葡萄牙语、阿拉伯语、俄语、日语、韩语和越南语。我们精心整理了该数据集，以确保其多样性和质量，同时利用了高质量的合成图像和真实世界的自然场景图像。这种组合确保了模型在各种语言环境下都能保持稳健的性能，并提高了模型对不同文本外观和环境条件的适应性。对于图表类型的数据，我们使用matplotlib、seaborn和plotly等可视化库合成了100万个样本，涵盖了条形图、关系图和热图等图表类型。对于表格数据，我们通过离线端到端表格识别模型处理了600万个真实世界样本，随后过滤掉了置信度低的表格、重叠的表格以及单元格密度不足的表格。

**Video Data**。为了增强模型对帧率 (FPS) 变化的视频数据的理解能力，我们在训练过程中动态采样 FPS，以使训练数据集中的 FPS 分布更加均匀。此外，对于时长超过半小时的视频，我们专门构建了一组长视频字幕，通过特定的合成流程合成多帧字幕。关于视频时间戳数据，我们同时采用了基于秒的格式和时分秒帧 (hmsf) 格式，以确保模型能够准确理解并输出各种格式的时间信息。

**Agent Data**。我们增强了 Qwen2.5-VL 的感知和决策能力，以构建其 Agent 能力。在感知方面，我们收集了移动端、网页端和桌面端的屏幕截图。我们使用合成数据引擎生成屏幕截图的标题和UI元素的上下文标注。标题标注任务帮助Qwen2.5-VL理解图形界面，而上下文标注任务则帮助它理解元素的外观和功能。在决策方面，我们首先将移动端、网页端和桌面端的操作统一为具有共享动作空间的函数调用格式。我们从开源数据中收集了一组带有标注的多步骤轨迹，这些轨迹由智能体框架在虚拟环境中合成，并重新格式化为函数格式。我们进一步通过人工标注员和模型标注员为每个步骤生成推理过程。具体来说，给定一个真实操作，我们会将其在屏幕截图中突出显示。然后，我们将全局查询以及操作前后的屏幕截图提供给标注者，并要求他们编写推理内容来解释此操作背后的意图。我们使用基于模型的过滤器来筛选掉低质量的推理内容。这些推理内容可以防止 Qwen2.5-VL 对真实操作进行过拟合，并使其在实际场景中更加稳健。

### 2.2.2 Training Recipe

我们使用 DataComp 数据集和一些内部数据集从头开始训练了一个视觉 Transformer (ViT) 模型，作为视觉编码器的初始化；同时，我们利用预训练的 Qwen2.5 大型语言模型 (LLM) 作为 LLM 组件的初始化。如表 2 所示，预训练过程分为三个不同的阶段，每个阶段都采用不同的数据配置和训练策略，以逐步提升模型的性能。

在第一阶段，仅训练视觉 Transformer（ViT），以提高其与语言模型的匹配度，为多模态理解奠定坚实的基础。此阶段的主要数据源包括图像描述、视觉知识和OCR数据。这些数据集经过精心挑选，旨在提升 ViT 提取有意义的视觉表征的能力，以便将其与文本信息有效整合。

在第二阶段，所有模型参数均被解冻，模型在多样化的多模态图像数据集上进行训练，以增强其处理复杂视觉信息的能力。此阶段引入了更复杂、推理密集型的数据集，例如交错数据、多任务学习数据集、视觉问答（VQA）数据集、多模态数学数据集、基于 Agent 的任务数据集、视频理解数据集和纯文本数据集。这些数据集增强了模型在视觉和语言模态之间建立更深层次联系的能力，使其能够处理日益复杂的任务。

在第三阶段，为了进一步提升模型对更长序列的推理能力，除了增加序列长度外，还加入了视频和基于 Agent 的数据。这使得模型能够更精确地处理更高级、更复杂的多模态任务。通过延长序列长度，模型获得了处理扩展上下文的能力，这对于需要长距离依赖关系和复杂推理的任务尤为有利。

为了应对图像尺寸和文本长度变化带来的挑战（这会导致训练过程中计算负载不平衡），我们采用了一种优化训练效率的策略。主要的计算成本来自 LLM 和视觉编码器。鉴于视觉编码器的参数相对较少，并且我们引入了窗口注意力机制以进一步降低其计算需求，我们着重于平衡 LLM 在不同GPU上的计算负载。具体而言，我们根据输入序列长度动态地将数据样本打包到LLM中，从而确保计算负载的一致性。在第一阶段和第二阶段，数据被统一打包成长度为8192的序列；而在第三阶段，序列长度增加到 32768，以适应模型处理更长序列能力的提升。

## 2.3 Post-training

Qwen2.5-VL 的后训练对齐框架采用了一种双阶段优化范式，包括有监督微调（SFT）和直接偏好优化（DPO）。这种分层对齐策略将参数高效的领域自适应与人类偏好蒸馏相结合，通过不同的优化目标，同时解决表征定位和行为改进的问题。

有监督微调 (SFT) 旨在通过有针对性的指令优化，弥合预训练表征与下游任务需求之间的差距。**在此阶段，我们采用 ChatML 格式来构建指令遵循数据，刻意偏离预训练数据模式**，同时保持与 Qwen2-VL 架构的一致性。这种格式转换实现了三项关键的调整：1) 为多模态多轮对话显式标注对话角色；2) 将视觉嵌入与文本指令结构化地注入；3) 通过格式感知打包保留跨模态位置关系。通过让模型接触基于此增强模式的精心整理的多模态指令-响应对，SFT 能够在保持预训练特征完整性的同时，实现高效的知识迁移。

### 2.3.1 Instruction Data

有监督微调 (SFT) 阶段采用精心构建的数据集，旨在提升模型在多种模态下的指令执行能力。该数据集包含约 200 万条数据，纯文本数据 (50%) 和多模态数据 (50%) 各占一半，后者包括图像-文本和视频-文本组合。多模态数据的加入使模型能够有效地处理复杂的输入。值得注意的是，尽管纯文本和多模态记录的比例相同，但由于嵌入了视觉和时间信息，多模态记录在训练过程中消耗的 token 和计算资源明显更多。该数据集主要由中文和英文数据组成，并补充了多语言的数据以支持更广泛的语言多样性。

该数据集的结构旨在反映不同复杂程度的对话，包括单轮和多轮互动。这些互动通过从单张图像输入到多张图像序列的各种场景进一步加以情境化，从而模拟真实的对话动态。问题数据主要来自开源存储库，并辅以精选的购买数据集和在线问题数据。这种组合确保了数据集的广泛覆盖，并增强了其代表性。

为了应对广泛的应用场景，该数据集包含针对通用视觉问答（VQA）、图像描述、数学问题求解、编码任务和安全相关查询的专用子集。此外，还构建了用于文档和光学字符识别（Doc 和 OCR）、场景构建、视频分析和智能体交互的专用数据集，以增强特定领域的专业能力。有关数据的详细信息，请参阅论文的相关章节。这种结构化且多样化的数据集构成确保了 SFT 阶段能够有效地将预训练表征与下游多模态任务的细微需求相匹配，从而提升模型的鲁棒性和上下文感知能力。

### 2.3.2 Data Filtering Pipeline

训练数据的质量是影响视觉语言模型性能的关键因素。开源数据集和合成数据集通常存在显著差异，往往包含噪声、冗余或低质量的样本。因此，严格的数据清洗和过滤流程对于解决这些问题至关重要。低质量数据会导致预训练表征与下游任务需求之间的匹配度欠佳，从而降低模型有效处理复杂多模态任务的能力。因此，确保高质量的数据对于实现稳健可靠的模型性能至关重要。

为了应对这些挑战，我们实现了一个**两阶段数据过滤流程**，旨在系统地提升有监督微调（SFT）数据集的质量。该流程包含以下几个阶段：

**Stage 1: Domain-Specific Categorization**。在初始阶段，我们采用 *Qwen2-VL-Instag* 模型（一种基于 Qwen2-VL-72B 的专用分类模型）对问答 (QA) 对进行层级分类。该模型将 QA 对组织成八个主要领域，例如编码和规划，每个领域又进一步细分为 30 个细粒度子类别。例如，主要领域“编码”细分为代码调试、代码生成、代码翻译和代码理解等子类别。这种层级结构有利于领域感知和子领域感知的过滤策略，使流程能够针对每个类别的特定特征优化数据清洗过程。因此，这提高了监督微调 (SFT) 数据集的质量和相关性。

**Stage 2: Domain-Tailored Filtering**。第二阶段涉及领域定制过滤，它融合了基于规则和基于模型的方法，以全面提升数据质量。鉴于文档处理、光学字符识别 (OCR) 和视觉接地等领域的多样性，每个领域可能都需要独特的过滤策略。下文概述了应用于这些领域的一般过滤策略。

**基于规则的过滤**采用预定义的启发式方法来消除低质量或有问题的条目。具体而言，对于与文档处理、OCR 和视觉识别任务相关的数据集，会识别并移除重复模式，以防止干扰模型的学习过程并确保最佳性能。此外，还会排除包含不完整、截断或格式不正确的响应的条目——这些响应在合成数据集和多模态环境中很常见。为了保持相关性并遵守伦理标准，还会丢弃不相关或可能导致有害输出的问题和答案。这种结构化的方法确保数据集符合伦理准则并满足特定任务的要求。

基于模型的过滤方法利用在 Qwen2.5-VL 系列数据集上训练的奖赏模型，进一步优化数据集。这些模型从多个维度评估多模态问答对。问题会根据其复杂性和相关性进行评估，仅保留那些难度适中且与上下文相关的示例。答案则根据其正确性、完整性、清晰度、与查询的相关性以及实用性进行评估。在视觉相关的任务中，尤其注重验证视觉信息的准确解读和运用。这种多维度评分确保只有高质量数据才能进入 SFT 阶段。

### 2.3.3 Rejection Sampling for Enhanced Reasoning

为了完善我们的结构化数据过滤流程，我们采用拒绝采样策略来优化数据集，并增强视觉语言模型（VLM）的推理能力。这种方法对于需要复杂推理的任务尤为重要，例如数学问题求解、代码生成和特定领域的视觉问答（VQA）。先前的研究表明，引入思维链（CoT）推理能够显著提升模型的推理性能。我们的后训练实验也证实了这一点，并强调了结构化推理过程对于获得高质量结果的重要性。

拒绝抽样过程始于添加了真实标注的数据集。这些数据集经过精心挑选，包含需要多步骤推理的任务，例如数学问题求解、代码生成和特定领域的视频问答（VQA）。我们使用 Qwen2.5-VL 模型的中间版本，将生成的响应与真实值进行比较。只有模型输出与预期答案匹配的样本才会被保留，从而确保数据集仅包含高质量、准确的示例。

将 CoT 推理应用于视觉语言模型的一个关键挑战在于其对文本和视觉模态的依赖性。中间推理步骤可能无法充分整合视觉信息，要么忽略相关的视觉线索，要么误解它们。为了解决这个问题，我们开发了基于规则和模型驱动的过滤策略来验证中间推理步骤的准确性。这些机制确保CoT过程中的每个步骤都能有效地整合视觉和文本模态。尽管做出了这些努力，实现最佳的模态对齐仍然是一个持续的挑战，需要进一步的研究和改进。

通过拒绝采样生成的数据显著提升了模型的推理能力。通过迭代优化数据集并移除低质量或错误样本，我们使模型能够从强调准确且连贯推理的高保真示例中学习。这种方法不仅增强了模型处理复杂任务的能力，也为未来视觉语言建模的改进奠定了基础。

### 2.3.4 Training Recipe

Qwen2.5-VL 的后训练处理流程分为两个阶段：有监督微调 (SFT) 和直接偏好优化 (DPO)，这两个阶段均在视觉 Transformer (ViT) 参数固定的情况下进行。在 SFT 阶段，模型在多种多模态数据上进行微调，这些数据包括图像-文本对、视频和纯文本，来源于通用视觉质量评估 (VQA)、拒绝采样以及文档和光学字符识别 (OCR)、接地、视频和智能体相关任务等专用数据集。DPO 阶段则专注于图像-文本和纯文本数据，利用偏好数据使模型与人类偏好保持一致，每个样本仅处理一次以确保高效优化。这种精简的流程增强了模型的跨模态推理能力和特定任务的性能，同时保持了与用户意图的一致性。

# 3.Experiments
