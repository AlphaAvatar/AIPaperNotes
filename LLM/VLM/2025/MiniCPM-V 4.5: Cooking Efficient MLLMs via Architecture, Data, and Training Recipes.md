论文链接：https://arxiv.org/pdf/2509.18154

代码链接：https://github.com/openbmb/MiniCPM-V

# 摘要

多模态大语言模型（MLLM）发展迅猛，代表着人工智能发展的前沿。然而，其训练和推理效率已成为制约 MLLM 普及和扩展的核心瓶颈。为了应对这些挑战，我们提出了MiniCPM-V 4.5，一个旨在实现高效高性能的 8B 参数模型。我们在模型架构、数据策略和训练方法方面进行了三项核心改进：统一的 3D 重采样器模型架构，用于对图像和视频进行高度紧凑的编码；统一的学习范式，无需繁重的数据工程即可实现文档知识和文本识别；以及混合强化学习策略，以提升模型在短推理和长推理模式下的熟练度。OpenCompass 评估的综合实验结果表明，MiniCPM-V 4.5的性能优于广泛使用的商业模型（例如GPT-4o-latest）以及规模更大的开源模型（例如Qwen2.5-VL 72B）。值得注意的是，其强大的性能是在显著提高效率的同时实现的。例如，在广泛采用的 VideoMME 基准测试中，MiniCPM-V 4.5 在 30B 大小以下的模型中取得了最先进的性能，仅使用了 Qwen2.5-VL 7B 的 46.7% 的 GPU 内存成本和 8.7% 的推理时间。

# 1.介绍

多模态大语言模型（MLLM）正迅速推进人工智能的前沿发展，使机器能够深入理解和推理不同模态的信息，例如文本和图像。然而，随着MLLM的演进，数据工程、训练和推理的成本也大幅增加。解决这一效率难题已成为研究和产业界关注的焦点，对于提高MLLM的普及性和可扩展性至关重要。

我们将此效率问题分解为三个核心方面：（1）**模型架构**。MLLM 的主要效率瓶颈在于高分辨率图像编码需要大量的视觉 tokne，这给视觉编码器和 LLM 带来了沉重的计算开销。在视频理解中，这个问题更加严重，现有模型即使在低帧率采样的情况下，也可能需要数千个 token 来编码一个短的低分辨率视频。例如，处理一个6秒、2帧/秒、分辨率仅为448×448的视频，Qwen2.5-VL 需要 1536 个 token，而InternVL3 则需要 3072 个 token。如此长的视觉 token 序列会导致GPU内存和计算速度方面过高的训练和推理成本。（2）**训练数据**。随着我们从传统网页数据中获取的新知识迅速枯竭，现代多模态学习模型（MLLM）的一个新基石是利用文档（例如科学论文和教科书）中的高质量多模态知识。这些文档通常以PDF格式存储，包含各个领域的多学科知识，并以交错的文本、图像和表格等多种布局组织。然而，大多数方法依赖于脆弱的外部解析工具将文档文件转换为用于训练的交错图像-文本序列。这些工具在复杂的布局中常常失效，导致知识学习错误或需要耗费大量数据工程精力来修复失效情况。(3) **训练方法**。强化学习（RL）通过在提供最终答案之前实现逐步的显式思考过程，展现出提升复杂推理能力的潜力。然而，这种性能提升往往以输出冗长为代价。即使是识别显而易见的物体这样的简单任务，大多数现有的思维模型也会产生过长的输出，导致训练和推理效率低下。例如，在综合性的 Opencompass 基准测试中，混合策略仅需 33.3% 的长推理样本即可达到完全以单一模式训练的峰值长推理性能。

为了应对这些挑战，MiniCPM-V 4.5 在模型架构、数据策略和训练方法方面引入了三项关键改进：（1）**统一的 3D 重采样器用于紧凑的图像和视频编码**。之前的 MiniCPM-V 系列模型通过 2D 重采样器对高分辨率图像实现了较高的压缩率（例如，比大多数 MLLM 低 4 倍）。为了进一步解决视频处理方面的架构效率问题，我们将 2D 重采样器扩展为 3D 重采样器，以联合压缩视频的时空信息。该模块可以将一段 6 秒、2 fps、448×448 分辨率的视频编码为仅 128 个视觉 token，与典型的 MLLM 相比，标记成本降低了 12 到 24 倍，从而能够高效地理解高帧率和长视频，并实现图像的统一编码。（2）**用于文档知识和 OCR 的统一学习范式**。我们提出了一种学习范式，使模型能够直接从文档图像中准确获取知识，从而无需依赖脆弱的外部解析器。通过动态地在文档中对不同噪声水平的文本区域进行损坏，并要求模型重建文本，该模型能够自适应地、恰当地在精确文本识别（文本大致可见时）和基于多模态上下文的知识推理（文本严重损坏时）之间切换。(3) **混合训练后策略**。与以往优化单一长推理模式的模型不同，我们开发了一种混合强化学习（RL）后训练策略，以同时支持高效的短推理模式和用于复杂任务的长推理模式。在强化学习训练过程中，我们在部署过程中随机交替使用这两种模式进行联合优化。这种方法不仅可以灵活控制短推理模式和长推理模式，还可以实现性能的相互提升。实验表明，对于这两种模式，我们都能在更少的训练样本下获得更好的推理性能。

OpenCompass 评估的综合实验结果表明，MiniCPM-V 4.5 的性能优于广泛使用的专有模型，例如 GPT-4o-latest，以及规模更大的开源模型，例如 Qwen2.5-VL 72B。值得注意的是，其强大的性能是在极高的效率下实现的。例如，借助高效的统一 3D 重采样器，MiniCPM-V 4.5 在 VideoMME 数据集上实现了与现有最​​先进的 MLLM 相当的性能，推理时间仅为后者的 9.9%。基于混合后训练策略，MiniCPM-V 4.5 在短推理和长推理模式下均表现出色，在 OpenCompass 评估中，其性能优于并发思维模型，而推理时间仅为后者的 42.9%-68.2%。

总而言之，我们的贡献如下：
- 我们开源了 MiniCPM-V 4.5，这是一个高效强大的 MLLM，支持高效的高帧率和长视频理解、可控的混合推理、强大的 OCR 和强大的文档解析能力。
- 我们引入了三项关键改进：统一的 3D 重采样器，用于高效的图像和视频编码；统一的文档知识和 OCR 学习范式；以及一种混合的后训练策略，可同时提高性能和效率。
- 综合实验证明了所提出的技术改进的有效性以及 MiniCPM-V 4.5 的性能。

# 2.Approach

在本节中，我们将介绍 MiniCPM-V 4.5 的方法，包括模型架构以及预训练、SFT 和 RL 的配方。

## 2.1 Architecture

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/95257116f8a6412b81939516534d8bd1.png)

如图 1 所示，MiniCPM-V 4.5 的架构包含三个主要模块：（1）一个轻量级视觉编码器，它采用特殊的分割策略灵活处理高分辨率图像。（2）一个统一的 3D 重采样器，它利用视觉信息中的时间冗余，将图像和视频编码成紧凑的特征。（3）一个 LLM 解码器，它能够理解图像、视频和文本，并生成文本输出。

### 2.1.1 The Unified 3D-Resampler

为了解决 MLLM 中图像和视频编码效率的瓶颈问题，我们将二维重采样器扩展为三维重采样器，从而**联合压缩视频的时空信息**。通过利用连续多个视频帧的时间冗余，我们实现了6倍的时间压缩率。

**Image Processing**。为了处理任意宽高比的高分辨率图像，我们采用了 LLaVAUHD 图像分割策略。对于每幅图像，我们根据输入分辨率估计理想的切片数，并选择切片分辨率与视觉编码器预训练设置偏差最小的分割方案。然后，我们使用可学习 qeury，并结合二维空间位置嵌入，通过交叉注意力机制为每个切片生成固定长度的序列。大多数现有的 MLLM 模型采用多层感知器（MLP）和像素解藕操作进行视觉压缩，通常需要256个视觉 token 来编码一幅448×448的图像。MiniCPM-V利用重采样器架构的灵活性，通过选择少量 query token，可以在保持良好性能的同时，显著提高视觉 token 的压缩率（例如，一幅448×448的图像只需64个 token）。

**Video Processing**。为了处理视频数据中显著的冗余信息，我们采用了一种时空联合压缩策略，以实现更高的压缩率。对于每个视频，我们首先沿时间维度将其分割成若干个包，每个包包含相邻的帧。直观地说，同一包内的视频帧通常共享高度冗余的视觉信息，这些冗余信息可以通过联合建模来识别和压缩。为此，我们利用交叉注意力机制，将每个包中视觉编码器的帧特征重采样为固定长度的特征序列。我们使用二维空间位置嵌入（类似于图像编码中使用的嵌入）和时间位置嵌入来增强可学习的 query。最终的视频表示是通过连接所有包中的 token 序列得到的。我们对每个视频最多采样 1080 帧，最大帧速率为 10。在训练过程中，我们会随机调整包的大小和帧速率以提高鲁棒性。这种设计也为推理阶段提供了灵活性，允许调整这些超参数以满足不同场景和设备的需求。

基于 3D 重采样器，MiniCPM-V 4.5 可实现 96 倍的视频 token 压缩率，将 6 帧 448×448 视频联合压缩为 64 个视频 token（大多数 MLLM 通常需要 1536-3072 个 token）。这意味着该模型可以在不增加 LLM 推理成本的情况下感知更多视频帧，从而具备强大的高帧率视频理解能力和长视频理解能力。


**Training Efficiency**。由于重采样机制的灵活性（与输入形状无关），我们可以使用同一个 3D 重采样器对图像和视频进行统一的视觉编码。这意味着图像编码和视觉编码共享相同的架构和权重，**因此，我们可以通过轻量级的 SFT 阶段高效地实现从 2D 重采样器到 3D 重采样器的扩展。此外，这也有助于从图像到视频的高效知识迁移**。例如，尽管我们没有专门收集视频训练数据，但在 MiniCPM-V 4.5 中观察到了良好的视频 OCR 能力。

> **Takeaway**。时空联合压缩可以实现更高的视觉压缩率。统一的架构只需极少的额外训练即可更高效地进行适配，并有助于将知识从图像迁移到视频。

## 2.2 Pre-training

我们的预训练过程旨在通过循序渐进的多阶段策略，系统地构建模型的基础能力。这包括精心设计的数据集以及一种用于文档知识和OCR学习的全新统一范式。

### 2.2.1 Pre-training Strategy

预训练包括三个渐进的阶段。每个阶段都会策略性地解冻不同的模型组件，并引入越来越复杂的数据，以优化学习效率。

**Stage 1**。我们首先进行预热阶段，仅训练 2D 重采样器模块，其他所有组件保持不变。此阶段使用 image-caption 数据，以最小的训练成本建立视觉模态和语言模态之间的初始对齐。

**Stage 2**。然后，我们解冻视觉编码器以增强感知基础能力。此阶段会训练包含 OCR 信息的数据和 image-caption 数据。由于此阶段的数据可能缺乏语言建模所需的流畅性或质量，因此LLM解码器在此阶段保持冻结状态。

**Stage 3**。在跨模态桥接和感知基础构建完成后，最后阶段使用我们最高质量的数据（包括纯文本语料库、图文交错样本、视频以及来自早期阶段的精选子集）对所有模型参数进行端到端训练。此时，我们解冻LLM解码器，以充分利用数据中的知识和技能，包括多图像推理和时间理解。**我们采用Warmup-Stable-Decay学习率调度器。在衰减阶段，我们逐步添加更多高质量指令和知识密集型数据**。

### 2.2.2 Pre-training Data

**Image Caption Data**。我们将大规模公共数据集（LAION-2B、COYO 等）与从网络抓取的精选中文图文对相结合。我们使用 CLIP 过滤掉低分辨率图像并去除不相关的图文对。为了丰富文本描述，我们对部分图文对采用基于 Capsfusion 的重描述方法，生成流畅且内容完整的描述。通过这种方式，我们将原始描述中蕴含的宝贵世界知识转化为更流畅的自然语言。我们使用 MLLM 为图像添加概念标签，并确保标签在不同语言和长尾概念之间均衡分布。

**Image-Text Interleaved Data**。图像-文本交错数据来源于 Common Crawl、OmniCorpus 和 MINT1T 数据库，对于上下文学习和多图像理解能力至关重要。我们应用过滤来确保数据质量，去除图像损坏或图像-文本比例失衡的样本。此外，我们还使用相关性过滤来确保有意义的多模态关联，并采用知识密度过滤来选择高质量的子集用于预训练的最终衰减阶段。

**OCR Data**。我们在早期预训练阶段合成 OCR 数据以增强基本的文本识别能力。我们参考 [22]，在自然场景中以各种颜色和字体组合渲染文本，并将真实世界的HTML源代码渲染成图像。

**Document Data**。我们从网络上收集文档，包括科学论文、学术报告、教科书等。这些数据知识密度高，且包含视觉上复杂的布局。

**Video Caption Data**。我们整合了多个公共数据集（WebVid、Vript、OpenVid），并为其补充了详细的视频字幕。这一多元化的数据集有助于培养时间视觉推理能力，而这对于视频理解至关重要。

### 2.2.3 Unified Paradigm for Document Knowledge and OCR Learning

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/59e81a1fbaa74f5cb4b476d7126f7916.png)

文档，例如科学论文、教科书和网页，是学习各种布局和获取跨领域多学科知识的重要资源。然而，大多数 MLLM 依赖于脆弱的外部解析器将文档 PDF 转换为用于训练的交错图像文本序列。这种**噪声大**且效率低下的过程常常引入结构性错误，或者需要大量的数据工程工作来修复错误案例。

OCR学习面临的另一个挑战是，虽然更强的**图像增强**可以创建更多样化、更难识别的样本，从而提升OCR的鲁棒性，但过度增强会导致文本难以区分。强迫模型从这种难以区分的视觉输入中生成真实文本通常会导致幻觉问题。因此，以往我们只能采用较小且安全的增强级别。

为了克服这两个挑战，我们提出了一种统一的训练范式，该范式直接从文档图像中学习，并以原始文本作为真值。**我们的核心观点在于，文档知识获取和文本识别之间的关键区别在于图像中文本的可见性。我们将这两种能力统一到一个学习目标中：从损坏的文档图像中预测原始文本**。通过动态地对文本区域进行不同程度的损坏，模型能够自适应地、正确地在精确文本识别（当文本可辨认时）和基于多模态上下文的知识推理（当文本严重模糊或被掩蔽时）之间切换，如图 2 所示。这消除了对脆弱的解析器的依赖，并防止了过度增强的 OCR 数据导致的错误。

具体来说，对于每份文档，我们将其文本区域的一个子集作为训练真值。然后，我们随机地对每个区域施加不同程度的损坏，从而创建不同的训练任务：
1. **Low Corruption (Augmented OCR)**。当对文本区域施加轻微噪声时，文本仍然可以识别，并且该模型可以通过文本识别有效地预测它们。
2. **Moderate Corruption (Integrated Inference)**。当文本区域受到大量噪声干扰时，单个字符会变得高度模糊，难以识别。因此，模型必须学习如何将受损区域中的噪声视觉线索与文档的高层上下文及其内部知识相结合，才能重建原始文本。
3.  **High Corruption (Contextual Inference and Document Knowledge Learning)**。由于文本区域被完全遮蔽，模型无法依赖字符级线索来预测缺失的内容。因此，模型只能从多模态上下文及其内部知识（包括其他文本、布局结构、图表、表格和图像）中推断信息。这直接培养了模型对文档层面的理解。

这种统一的方法能够带来更高效、更具弹性的学习过程。通过直接从文档的视觉和文本结构中学习，我们避免了构建复杂的文档解析流程，并防止了脆弱的解析器引入潜在的噪声。此外，这种范式允许我们在同一训练批次中流畅地结合知识学习和OCR目标，从而最大限度地利用数据，并生成一个能够胜任各种文档理解任务的通用模型。

> **Takeaway**
> 1. 通过有选择地冻结参数，可以在不完善的异构数据源上构建基础技能。
> 2. 文档图像文本上的简单动态视觉损坏​​可以有效地将知识学习、强大的 OCR 和上下文推理统一到一个学习目标中。

## 2.3 Supervised Fine-tuning

有监督微调（SFT）阶段旨在激活模型在各种任务上的能力，并为强化学习做好准备。此外，我们在此阶段将二维重采样器扩展为统一的三维重采样器，以提高视频数据的压缩效率。

### 2.3.1 Supervised Fine-tuning Strategy

我们首先训练通用交互能力，然后培养高级推理和时间理解方面的专门技能。

**Stage 1: General SFT**。此阶段旨在激活预训练期间积累的广泛知识，并将其与人类指令相匹配。通过在高质量的指令-响应数据组合上进行微调，模型能够熟练地处理多模态交互。为了防止纯文本性能下降并提高训练稳定性，我们在训练数据集中加入了10%的高质量纯文本数据。

**Stage 2: Long-CoT & 3D-Resampler**。在前一阶段构建的通用基础上，我们进一步培养了支持长时推理模式、高帧率和长视频理解的专项技能。首先，我们通过在SFT数据中引入 Long-CoT 预热指令来解锁高级推理能力。这促使模型执行明确的逐步思考过程，并融入反思和回溯等认知模式，这些模式对于长时推理模式至关重要。其次，我们通过将架构从2D升级到3D重采样器，并引入高帧率和长视频数据来增强其时间理解能力。由于采用了统一的设计，我们发现只需少量高质量视频数据即可高效地实现这种升级。

### 2.3.2 Supervised Fine-tuning Data

**STEM Data**。为了提升 STEM 领域的推理能力，我们从在线教育网站收集了涵盖物理、化学、生物、金融、计算机科学等学科的高中及以上跨学科问题数据集。为了确保数据质量，我们采用了两阶段过滤流程。首先，我们仅保留那些视觉依赖性高的样本（即，无法在不依赖图像信息的情况下求解的样本）。其次，我们进行一致性检查以验证答案的正确性。对于每个剩余的样本，我们使用强大的 MLLM 进行拒绝采样，从而获得一个干净的推理过程。

**Long-tail Knowledge Data**。为了解决模型在不太常见的主题上经常失效的长尾问题，我们整合了来自维基百科的长尾知识，以合成高质量的多模态指令遵循数据。具体来说，对于每个实体页面，我们使用强大的 MLLM 构建多模态指令和答案，并保留视觉依赖性高的样本。

**Long-CoT Data**。Long-CoT 数据使模型能够获取长推理模式所需的推理模式。我们的数据来自 OpenThoughts 和内部开发的流程。我们通过筛选早期模型难以处理的题目来识别具有挑战性的提示。我们的试点研究表明，专注于具有挑战性的问题是发展稳健推理能力的关键，而不是记忆简单的模式。每个回答随后都要经过多阶段验证：我们验证其正确性，使用 RLAIF-V 进行声明级事实验证以评估其可信度，并过滤掉无意义的重复。最后，通过重写来增强已验证回答的多样性。

> **Takeaway**。筛选掉简单的 prompt，专注于具有挑战性的问题，对于有效的 Long-CoT 热身至关重要。

## 2.4 Reinforcement Learning

强化学习阶段旨在提升推理性能、实现可控推理模式并提高可信度。为了提供高效的通用领域奖赏，我们将简单案例的规则验证奖赏与来自 RLPR 的基于概率的通用奖赏相结合，用于处理复杂答案，并添加了校准后的偏好奖赏。我们采用了一种混合强化学习策略，允许在短推理模式和长推理模式之间灵活切换。此外，我们还集成了 RLAIF-V 以减少幻觉。

### 2.4.1 Reinforcement Learning Data

### 2.4.2 Reward Quality Control

### 2.4.3 Hybrid Reinforcement Learning

### 2.4.4 Reward Shaping

### 2.4.5 RLAIF-V

视觉幻觉仍然是多模态大模型（MLLMs）的关键限制，尤其是在需要高可靠性的应用中。为了解决这一挑战，我们引入 RLAIF-V，通过可扩展的 AI 反馈对齐，使模型的响应在事实层面更加贴合视觉输入。值得注意的是，我们将这一方法扩展到了视频输入，在该领域幻觉问题尤为突出。

**响应采样**。 我们首先在相同的生成条件下，从策略模型中采样多个响应。这一策略确保能够集中评估事实准确性，避免不同模型之间的分布不匹配。

**反馈收集**。 我们将复杂的回答分解为可验证的原子级断言，并对每个断言进行独立验证。这样就把复杂的长文本评估转化为更简单的断言级验证，从而解决整体评估中的固有困难，并提高事实性评估的精度。随后，我们基于聚合后的断言验证得分构建偏好对，其中包含更少事实错误的回答会被优先选择。

**偏好学习**。 最终生成的偏好数据集（涵盖图像和视频模态）被用于通过 DPO 对模型进行训练。这一阶段在事实准确性至关重要的视觉任务中尤为有效，同时不影响回答质量或自然语言流畅性。

> **Takeaway*
> 1. 对简单响应给予基于规则的奖赏，对复杂自然语言反应给予基于概率的奖赏，两者相结合，可以为各种任务建立可靠的奖赏系统。
> 2. 混合强化学习能够实现长推理模式和短推理模式之间的跨模式泛化。

# 3.Experiments