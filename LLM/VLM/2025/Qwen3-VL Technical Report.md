论文链接：https://arxiv.org/pdf/2511.21631

代码链接：https://github.com/QwenLM/Qwen3-VL

# 摘要

我们推出了 Qwen3-VL，这是迄今为止 Qwen 系列中最强大的视觉语言模型，在广泛的多模态基准测试中均取得了卓越的性能。它原生支持高达 256K 个 token 的交错上下文，能够无缝集成文本、图像和视频。该模型家族包含密集型（2B/4B/8B/32B）和混合专家型（30B-A3B/235B-A22B）变体，以满足不同的延迟-质量权衡需求。Qwen3-VL 具备三大核心优势：(i) 显著增强的纯文本理解能力，在多个方面超越了同类纯文本骨干模型；(ii) 强大的长上下文理解能力，原生支持 256K 个 token 的窗口，适用于文本和交错多模态输入，能够忠实地保留、检索和交叉引用长文档和视频中的信息； (iii) 针对单图像、多图像和视频任务，我们实现了先进的多模态推理，并在 MMMU 和视觉数学基准测试（例如 MathVista 和 MathVision）等综合评估中展现了领先的性能。在架构方面，我们引入了三项关键升级：(i) 增强了交错式 MRoPE，以增强图像和视频的时空建模能力；(ii) 集成了 DeepStack，有效利用多级 ViT 特征来加强视觉与语言的对齐；(iii) 针对视频的基于文本的时间对齐，从 T-RoPE 演进到显式文本时间戳对齐，以实现更精确的时间定位。为了平衡纯文本和多模态学习目标，我们应用了平方根重加权，在不影响文本处理能力的前提下提升了多模态性能。我们将预训练扩展到 256K 个 token 的上下文长度，并将后训练分为非思考和思考两种变体，以满足不同的应用需求。此外，我们为后训练阶段分配了额外的计算资源，以进一步提升模型性能。在相同的 token 预算和延迟约束下，Qwen3-VL 在密集架构和混合专家 (MoE) 架构中均取得了优异的性能。我们设想 Qwen3-VL 可作为实际工作流程中图像推理、智能决策和多模态代码智能的基础引擎。

# 1.介绍

近年来，视觉语言模型（VLM）取得了实质性进展，从基础的视觉感知发展到跨图像和视频的高级多模态推理。VLM的快速发展催生了下游应用领域的蓬勃发展，例如长上下文理解、STEM 推理、GUI 理解与交互以及智能体工作流程。至关重要的是，这些进步绝不能削弱底层大语言模型（LLM）的语言能力；多模态模型在语言基准测试中应达到或超越其纯文本模型的水平。

本报告介绍了 Qwen3-VL 及其在通用和高级应用方面的最新进展。基于 Qwen3 系列，我们实例化了四个密集模型（2B/4B/8B/32B）和两个混合专家（MoE）模型（30B-A3B / 235B-A22B），每个模型均使用高达 256K 个 token 的上下文窗口进行训练，以实现长上下文理解。通过优化训练语料库和训练策略，我们在视觉语言（VL）训练过程中保留了底层语言学习模型（LLM）的语言能力，从而显著提升了整体性能。我们发布了非思维变体和思维变体；后者展现出显著更强的多模态推理能力，在复杂推理任务中取得了更优异的性能。

我们首先介绍架构改进，这些改进包含三个部分：1）**增强的位置编码**。在 Qwen2.5-VL 中，我们使用 MRoPE 作为文本和视觉的统一位置编码方案。我们观察到，将嵌入维度分块为时间 (t)、水平 (h) 和垂直 (w) 组会导致频谱不平衡，并阻碍长视频的理解。因此，我们采用了一种交错式 MRoPE，将 t、h 和 w 均匀分布在低频段和高频段，从而产生更忠实的位置表示。2）**用于跨层融合的 DeepStack**。为了加强视觉-语言对齐，我们引入了开创性的 DeepStack 机制。来自视觉编码器不同层的视觉 token 通过轻量级残差连接路由到相应的语言层，从而在不引入额外上下文长度的情况下增强多级融合。3）**显式视频时间戳**。我们用显式时间戳 token 来标记帧组，从而取代了 Qwen2.5-VL 中使用的基于位置编码的绝对时间对齐方式，提供了一种更简单、更直接的时间表示。此外，在优化方面，我们将基于样本的损失函数改为基于 token 的平方根归一化损失函数，这能更好地平衡训练过程中文本数据和多模态数据的贡献。

为了构建更强大、更稳健的视觉语言基础模型，我们从质量、多样性和结构三个方面全面改进了训练数据。关键升级包括：增强字幕监督、扩展 omni 识别和 OCR 覆盖范围、利用3D/空间推理实现规范化定位，以及新增代码、长文档和时间定位视频语料库。此外，我们还融入了思维链推理和高质量、多样化的GUI-智能体交互数据，以连接感知、推理和行动。这些创新共同实现了更强大的多模态理解、更精准的定位以及工具增强智能。

我们的训练流程包含两个阶段：**预训练**和**后训练**。预训练分为四个阶段：首先是预热对齐阶段，该阶段仅更新合并层（视觉-语言投影），而模型的其余部分保持冻结；随后是全参数训练，上下文窗口逐渐增大，序列长度分别为 8K、32K 和 256K。后训练包含三个阶段：（i）在长思维链数据上进行监督式微调；（ii）从更强的教师模型中提炼知识；（iii）强化学习。

上述创新使 Qwen3-VL 不仅具备强大的视觉语言基础模型能力，而且成为一个灵活的真实世界多模态智能平台，能够无缝集成感知、推理和行动，并应用于各种不同的领域。在接下来的章节中，我们将介绍模型架构、训练框架以及大量的评估结果，这些结果证明了 Qwen3-VL 在文本、视觉和多模态推理基准测试中均表现出稳定且具有竞争力的性能。

# 2.Model Architecture

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/56fb0ee809a54622a7abdaaa84d35d7e.png)


Qwen3-VL沿用了 Qwen2.5-VL 的架构，采用三模块结构，包括视觉编码器、基于多层感知器（MLP）的视觉-语言融合器和大语言模型（LLM）。图 1 展示了详细的模型结构。

**Large Language Model**：Qwen3-VL 包含三个密集型变体（Qwen3-VL-2B/4B/8B/32B）和两个 MoE 变体（Qwen3-VL-30B-A3B 和 Qwen3-VL-235B-A22B），所有变体均基于 Qwen3 主干网络构建。旗舰模型 Qwen3-VL-235B-A22B 总共有 235B 个参数，每个 token 激活 22B 个参数。它在广泛的多模态任务中均优于大多数视觉语言模型，并且在大多数语言基准测试中超越了其纯文本版本。

**Vision Encoder**：我们采用 SigLIP-2 架构作为视觉编码器，并使用基于官方预训练检查点初始化的动态输入分辨率对其进行持续训练。为了有效适应动态分辨率，我们采用 2D-RoPE 算法，并根据输入尺寸插值绝对位置嵌入，方法与 CoMP 类似。具体而言，我们默认使用 SigLIP2-SO-400M 变体，而对于小尺寸 LLM（2B 和 4B），则使用 SigLIP2-Large (300M) 变体。

**MLP-based Vision-Language Merger**：与 Qwen2.5-VL 类似，我们使用双层 MLP 将视觉编码器输出的 2×2 视觉特征压缩成一个与 LLM 隐藏维度对齐的单个视觉 token。此外，我们还部署了专门的合并器来支持 DeepStack 机制，其详细信息请参见第 2.2 节。

## 2.1 Interleaved MRoPE

Qwen2-VL 引入了 MRoPE 来对多模态输入进行位置信息建模。在其原始版本中，嵌入维度被划分为时间 (t)、水平 (h) 和垂直 (w) 三个子空间，每个子空间都被赋予不同的旋转频率。这导致了频谱不平衡，后续研究表明，这种不平衡会降低长视频理解基准测试的性能。为了解决这个问题，我们重新设计了频率分配，将 t、h 和 w 分量交错分布在嵌入维度上。这确保了每个时空轴在低频和高频频段上都得到均匀的表示。由此产生的平衡频谱缓解了原有的频谱偏差，并显著提高了视频的长距离位置建模性能。

## 2.2 DeepStack

我们从 DeepStack 中汲取灵感，并将视觉 token 注入到 LLM 的多个层中。与 DeepStack 原始方法堆叠来自多尺度视觉输入的 token 不同，我们扩展了 DeepStack，使其能够从 Vision Transformer (ViT) 的中间层提取视觉 token。这种设计保留了丰富的视觉信息，涵盖了从低级到高级的各种表示形式。

具体而言，如图 1 所示，我们从视觉编码器的三个不同层级中选择特征。随后，专用的视觉-语言融合模块将这些多级特征投影到视觉 token 上，然后将这些视觉标记直接添加到前三个 LLM 层的相应隐藏状态中。

## 2.3 Video Timestamp

在 Qwen2.5-VL 中，我们采用了一种时间同步的 MRoPE 变体来赋予模型时间感知能力。然而，我们发现这种方法存在两个关键局限性：（1）由于该方法将时间位置 ID 直接与绝对时间关联，因此对于长视频，其时间位置 ID 过大且过于稀疏，从而降低了模型理解长时间上下文的能力。（2）在这种方案下进行有效学习需要对各种帧速率 (fps) 进行广泛且均匀分布的采样，这显著增加了训练数据构建的成本。

为了解决这些问题，我们采用了一种基于文本 token 的时间编码策略，其中每个视频时间块都带有一个以格式化文本字符串形式表示的时间戳，例如 $\texttt{<3.0 seconds>}$。此外，在训练过程中，我们生成秒格式和 HMS（小时:分钟:秒）格式的时间戳，以确保模型能够学习解释不同的时间码表示。虽然这种方法会略微增加上下文长度，但它使模型能够更有效、更精确地感知时间信息，从而有助于执行诸如视频定位和密集字幕生成等时间感知视频任务。

# 3. Pre-Training

## 3.1 Training Recipe

我们首先基于预训练的 SigLIP-2 模型，通过动态分辨率的持续训练来增强视觉编码器。整体 Qwen3-VL 模型采用三模块架构，包括该视觉编码器、基于多层感知器（MLP）的视觉-语言融合器以及 Qwen3 大语言模型（LLM）骨干网络。基于此架构，我们的预训练方法系统地分为四个阶段，旨在逐步构建从基本对齐到长上下文理解的能力。表1概述了这些阶段。

**Stage 0: Vision-Language Alignment**。初始阶段（S0）的重点在于高效地弥合视觉编码器和 LLM 之间的模态鸿沟。至关重要的是，在此阶段仅训练 MLP 融合器的参数，而视觉编码器和 LLM 主干网络均保持冻结状态。我们使用了一个包含约 670 亿个 token 的精选数据集，该数据集由高质量的图像-描述对、视觉知识集合和光学字符识别（OCR）数据组成。所有训练均采用 8192 个序列长度。这种先对齐后训练的方法为跨模态理解奠定了坚实的基础，然后再进行全参数训练。

**Stage 1: Multimodal Pre-Training**。完成初始对齐后，第一阶段 (S1) 过渡到全参数多模态预训练。在此阶段，我们解冻所有模型组件——视觉编码器、融合器和语言学习模型 (LLM)——以进行联合端到端训练。该模型在一个包含约 1 万亿 (1T) 个 token 的庞大且多样化的数据集上进行训练。为了保持 LLM 强大的语言能力，数据混合由视觉语言 (VL) 数据和纯文本数据组成。VL 部分内容丰富多样，包括交错的图像-文本文档、视觉定位任务、视觉问答 (VQA)、STEM 领域的数据以及少量视频数据，以引入时间理解。序列长度保持在 8,192。

**Stage 2: Long-Context Pre-Training**。第二阶段 (S2) 旨在显著提升模型的上下文处理能力。该阶段的关键变化在于将序列长度增加至 32,768，是之前的四倍，同时所有模型参数仍然可训练。训练在一个包含约 1T 个 token 的数据集上进行，并调整了数据混合比例以支持长上下文任务。纯文本数据的比例有所增加，以增强模型对长文本的理解能力；而剩余的 VL 数据则包含大量视频和面向智能体的指令跟随数据。该阶段对于使模型能够处理和推理更长的视频以及复杂的多步骤任务至关重要。

**Stage 3: Ultra-Long-Context Adaptation**。最后阶段（S3）是一个专门设计的阶段，旨在将模型的上下文窗口推向其运行极限。在此阶段，我们将序列长度大幅增加至 262,144。模型使用一个更聚焦的、包含 1000 亿个 token 的数据集进行训练，该数据集是专门为此目的精心策划的。该数据集包含纯文本数据和视频语言（VL）数据，并着重训练长视频和长文档理解任务。这一最终调整巩固了 Qwen3-VL 在处理和分析超长序列输入方面的能力，这对于综合文档分析和长视频摘要等应用至关重要。

## 3.2 Pre-Training Data

### 3.2.1 Image Caption and Interleaved Text-Image Data

为了构建一个稳健的通用视觉语言理解基础模型，我们显著扩展并改进了两种核心数据模态：图像-描述对和交错的文本-图像序列。我们的策略强调高质量、多样化且语义丰富的多模态基础，并辅以专门构建的模型和严格的过滤流程。

**Image Caption Data**。我们从网络资源中收集了大量当代以中英文为主的多语言图像-文本对语料库，并应用以专门针对图像重述进行微调的 Qwen2.5-VL-32B 模型为核心的多阶段优化流程。该模型利用每张图像关联的原始文本，生成更全面、流畅、细致的图像描述，在丰富视觉元素（例如物体属性、空间布局和上下文语义）描述的同时，也提升了文本部分的语言质量和信息量。

去重操作仅针对重新添加字幕的文本进行，并使用语义相似度度量，从而确保在不牺牲视觉多样性的前提下去除冗余样本。为了进一步增强对代表性不足概念的覆盖，我们对视觉嵌入应用聚类，以识别数据分布中的稀疏区域并进行针对性的数据增强。最终得到一个高保真度的字幕数据集，该数据集在规模、多样性和描述粒度方面实现了平衡。

**Interleaved Text-Image Data**。我们收集了来自近期中文和英文网站的各种真实世界的多模态文档。所有文档均使用基于 Qwen 的轻量级评分器进行领域分类，该评分器针对细粒度领域识别进行了微调。基于跨领域的验证实验，我们使用相同的高效评分器系统地排除有害或低价值类别，例如广告、促销内容和标题党，从而过滤掉不理想的样本。

对于书籍规模的交错数据，我们采用经过微调的 Qwen2.5-VL-7B 模型进行高精度多模态解析，精确提取文本并将其与嵌入的图形、图表和照片对齐。为了实现超长上下文建模，我们构建了一个专门的子集，将连续页面合并成最多 256K 个 token 的序列，从而保留了自然的页面顺序和多模态连贯性。在预处理过程中，我们实施了严格的质量控制：（i）移除纯文本或低对齐度的片段；（ii）对于超长书籍序列，我们要求达到最小页数和最小图像文本比，以确保整个上下文中存在有意义的视觉-文本交互。最终得到一个干净、多样化且布局感知的交错语料库，该语料库针对具身理解和长程多模态推理进行了优化。

### 3.2.2 Knowledge

世界知识对于多模态大语言模型（MLLM）至关重要，它能够帮助模型在各种下游任务中实现稳健的视觉理解、扎根推理和实体感知生成。为了使 Qwen3-VL 全面掌握现实世界和虚构世界的概念，我们构建了一个大规模的预训练数据集，该数据集以定义明确的实体为中心，涵盖十几个语义类别，包括动物、植物、地标、食物以及车辆、电子产品和服装等日常用品。

现实世界中的实体遵循长尾分布：显著的概念频繁出现并具有高质量的标注，而大多数实体则很少出现。为了解决这种不平衡问题，我们采用了一种基于重要性的采样策略。高显著性实体被赋予更高的采样权重，以确保足够的学习信号；而低​​显著性实体则以较小的比例被包含，以在不使训练过程过载的情况下保持广泛的覆盖范围。这种方法有效地平衡了数据质量、实用性和多样性。

所有保留的样本都经过多阶段的精细化处理。除了标准的噪声和错位过滤之外，我们还用更丰富的、由 LLM 生成的描述替换了原始或稀疏的标题（例如通用替代文本）。这些增强的标题不仅识别了主要实体，还描述了其视觉属性、周围环境、空间布局以及与其他物体或人物的互动，从而提供了更完整、更贴近实际的文本表征。

这些努力共同产生了知识丰富、具有上下文感知和以区分为中心的训练信号，显著增强了 Qwen3-VL 在真实场景中识别、推理和准确描述视觉概念的能力。

### 3.2.3 OCR, Document Parsing and Long Document Understanding

**OCR**。为了提升在真实图像上的 OCR 性能，我们使用由粗到精的流程，构建了一个包含3000万个内部采集样本的数据集。该流程通过整合来自 OCR 专用模型的伪标签和 Qwen2.5-VL 的优化结果，无需任何人工标注，即可提升 OCR 标注的质量。除了 Qwen2.5-VL 支持的10种语言（不包括中文和英文）之外，我们还纳入了另外29种语言，最终合成了约3000万个高质量的多语言 OCR 样本，并整理了超过100万张内部真实世界多语言图像。

**Document Parsing**。为了进行文档解析，我们从 Common Crawl 收集了 300 万份 PDF 文件，均匀分布在 10 种文档类型中（每种类型 30 万个样本），此外还收集了 400 万份内部文档。首先，我们使用内部开发的布局模型预测文本和非文本区域的阅读顺序和边界框；然后，Qwen2.5-VL-72B 执行特定区域识别。最后，我们将输出结果重新组装成位置感知且布局对齐的解析数据。

为了确保对异构格式进行稳健的解析，我们设计了一个统一的标注框架，支持两种表示形式：
-  QwenVL-HTML，其中包括细粒度的元素级边界框；
- QwenVL-Markdown，其中仅图像和表格进行本地化，表格采用 LaTeX 编码。

我们构建了一个大规模的、带有精确标注的合成 HTML 语料库，并将其系统地转换为 Markdown 格式。为了进一步提升模型的泛化能力，我们对大量真实文档生成伪标签，并对其进行质量筛选。最终的训练集结合了合成数据和高质量的伪标签数据，从而增强了模型的可扩展性和鲁棒性。

**Long Document Understanding**。为了增强模型理解多页PDF（通常跨越数十页）的能力，我们利用了一个大规模的长文档数据语料库。首先，我们通过合并单页文档样本来合成长文档解析序列。在每个序列中，多个页面图像被放置在开头，随后是它们对应的文本，这些文本是通过OCR或HTML解析获得的。其次，我们构建了长文档视觉问答（VQA）数据。具体来说，我们对高质量的多页PDF进行采样，并生成一组多样化的VQA示例，这些示例要求模型能够跨多个页面和异构文档元素（例如图表、表格、图形和正文）进行推理。我们精心平衡了问题类型的分布，并确保支持性证据来自各种模态和布局组件，从而促进在扩展上下文中进行稳健、可靠且多跳推理。

### 3.2.4 Grounding and Counting

### 3.2.5 Spatial Understanding and 3D Recognition

## 3.2.6 Code

### 3.2.7 Video

### 3.2.8 Science, Technology, Engineering, and Mathematics (STEM)

# 4.Post-Training

## 4.1 Training Recipe

我们的后训练流程是一个三阶段过程，旨在提升模型的指令执行能力、增强其推理能力并使其更符合人类偏好。各阶段的具体数据和方法将在后续章节中详细介绍。

**Supervised Fine-Tuning (SFT)**。第一阶段旨在培养指令遵循能力并激活潜在的推理技能。该阶段分为两个步骤：初始阶段使用 32k 的上下文长度，随后扩展到 256k 的上下文窗口，重点关注长文档和长视频数据。为了满足不同的需求，我们将训练数据分为两种格式：一种是用于非思维模型的标准格式，另一种是用于思维模型的“思维链”（CoT）格式，后者明确地对推理过程进行建模。

**Strong-to-Weak Distillation**。第二阶段采用知识蒸馏，将强大的 teacher 模型的能力传递给 student 模型。关键在于，我们使用纯文本数据进行知识蒸馏，以微调LLM主干模型。该方法被证明非常有效，显著提升了学生在以文本为中心的任务和多模态任务中的推理能力。

**Reinforcement Learning (RL)**。最后阶段利用强化学习进一步提升模型性能和一致性。该阶段分为推理强化学习和通用强化学习。我们应用大规模强化学习，涵盖包括但不限于数学、光学字符识别、定位和指令跟随等一系列文本和多模态领域，以提升模型的细粒度能力。

## 4.2 Cold Start Data

## 4.3 Strong-to-Weak Distillation

我们采用 Qwen3 中描述的由强到弱蒸馏流程来进一步提高轻量级模型的性能。该蒸馏过程包含两个主要阶段：
-  **Off-policy Distillation**。在第一阶段，teacher 模型生成的输出被整合起来，以实现响应蒸馏。这有助于轻量级 student 模型获得基本的推理能力，为后续的策略训练奠定坚实的基础。
- **On-policy Distillation**。在第二阶段，student 模型根据提供的提示生成响应。这些符合策略的序列随后用于微调 student 模型。我们通过最小化 KL 散度来对齐 student 和 teacher 预测的logits。

## 4.4 Reinforcement Learning

## 4.5 Thinking with Images

## 4.6 Infrastructure

我们在阿里云的 PAI-凌君人工智能计算服务上训练 Qwen3-VL 系列模型，该服务提供人工智能和高性能计算等计算密集型场景所需的高性能计算能力。

在预训练阶段，该系统采用基于 MegatronLM 框架的混合并行策略，集成了张量并行 (TP)、流水线并行 (PP)、上下文并行 (CP)、专家并行 (EP) 和 ZeRO-1 数据并行 (DP)。这种配置在模型规模、计算负载和通信开销之间实现了精细的平衡，从而能够实现高硬件利用率，并维持高吞吐量和低通信延迟——即使在高达 10,000 个 GPU 的规模下也是如此。

为了进行本地部署和性能评估，我们采用了基于 vLLM 或 SGLang 的部署策略。vLLM 利用 PagedAttention 实现高效的内存管理和高吞吐量的推理，而 SGLang 则擅长结构化数据生成和处理复杂的提示信息。这两个后端结合起来，能够提供稳定、高效且灵活的模型推理能力，从而实现高效的推理和评估。

# 5.Evaluation
