论文链接：https://arxiv.org/pdf/2508.06471

代码链接：https://github.com/zai-org/GLM-4.5

# 摘要

我们推出 GLM-4.5，这是一个开源的**混合专家 (MoE) 大语言模型**，总参数量达 355B，激活参数量达 32B，采用**混合推理**方法，支持思考和直接响应模式。通过对 23T 语料进行多阶段训练，并结合专家模型迭代和强化学习进行全面的后训练，GLM-4.5 在 Agent、推理和编码 (ARC) 任务中取得了优异的表现，在 TAU-Bench 上得分高达 70.1%，在 AIME 24 上得分高达 91.0%，在 SWE-bench Verified 上得分高达 64.2%。GLM-4.5 的参数量远少于其他竞品，在所有评估模型中总体排名第三，在 Agent 基准测试中排名第二。我们发布了 GLM-4.5（355B 参数）及其精简版本 GLM-4.5-Air（106B 参数），以推动推理和 Agent 人工智能系统的研究。代码、模型和更多信息可在 https://github.com/zai-org/GLM-4.5 上找到。

# 1.介绍

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/da97dbf13c704afcb727e736d4370fe3.png)

大语言模型 (LLM) 正在迅速从通用知识库演变为通用问题解决工具。其终极目标，通常与通用人工智能 (AGI) 相关，是创建跨领域、拥有人类水平认知能力的模型。这需要模型具备统一的复杂问题解决能力、泛化能力和自我提升能力，超越特定任务的卓越表现。

随着 LLM 越来越融入现实世界，提升实际生产力和解决复杂专业任务的关键在于培养特定的核心能力。我们确定了三种关键且相互关联的能力，作为衡量真正通用模型的标准：与外部工具和现实世界交互的 **Agent 能力**；解决数学和科学等领域多步骤问题的复杂**Reasoning 能力**；以及处理现实世界软件工程任务的**Coding 能力**。虽然像 OpenAI 的 o1/o3 和 Anthropic 的 Claude Sonnet 4 这样的先进专有模型已经在特定的 ARC 领域（例如数学推理或代码修复）展现出突破性的性能，但一个能够在所有三个领域都表现卓越的单一、强大的开源模型仍然难以捉摸。

本文介绍了两个新模型：GLM-4.5 和 GLM-4.5-Air，旨在统一所有不同的功能。新模型全面超越现有的开源 LLM 模型，在 Agent、推理和编码任务中均取得显著提升。GLM-4.5 和 GLM4.5-Air 均采用混合推理模式：用于复杂推理和代理任务的思考模式，以及用于即时响应的非思考模式。GLM-4.5 是我们的第一个 MoE 模型，拥有 355B 个总参数和 32B 个激活参数。GLM-4.5 在以下 ARC 基准测试中表现出色：
- **Agentic**：GLM-4.5 在 TAU-Bench 上的得分为 70.1%，在 BFCL v3 上的得分为 77.8%，与 Claude Sonnet 4 相当。对于网页浏览 Agent，GLM-4.5 在 BrowseComp 上的得分为 26.4%，明显优于 Claude Opus 4（18.8%），并接近 o4-mini-high（28.3%）。
- **Reasoning**：GLM-4.5 在一系列具有挑战性的推理基准测试中表现出色，在 AIME 24 上取得了 91.0% 的准确率，在 GPQA 上取得了 79.1% 的准确率，在 LiveCodeBench (2407-2501) 上取得了 72.9% 的准确率，在 HLE (人类的最后考试) 上取得了 14.4% 的准确率。
- **Coding**：GLM-4.5 在 SWE-bench Verified 上的得分为 64.2%，在 Terminal-Bench 上的得分为 37.5%，优于 GPT-4.1 和 Gemini-2.5-pro，接近 Claude Sonnet 4。

GLM-4.5-Air 是一个较小的 MoE 模型，拥有 106B 个参数。它在 100B 规模的模型中实现了显著的飞跃，匹敌甚至超越了 Qwen3-235B-A22B 和 MiniMax-M1。

图 1 展示了 GLM-4.5 在 12 个基准测试中，涵盖 Agent、推理和编码 (ARC) 任务的平均性能。总体而言，GLM-4.5 排名第三，GLM-4.5-Air 排名第六。在 Agent 任务中，GLM-4.5 排名第二，仅次于 OpenAI o3。在编码任务中，GLM-4.5 排名第三，接近 Claude Sonnet 4。值得注意的是，GLM-4.5 的参数效率极高，其参数仅为 DeepSeek-R1 的一半，仅为 Kimi K2 的三分之一。图 2 展示了 GLM-4.5 在 SWE-bench Verified 上的得分与不同开源模型的模型参数之间的关系，其中 GLM-4.5 和 GLM-4.5-Air 位于帕累托前沿。更多评估结果详见第 4 节。

GLM-4.5 和 GLM-4.5-Air 均可在 Z.ai 和 BigModel.cn 获取，也可在 https://huggingface.co/zai-org/GLM-4.5 上找到开源模型。我们还在 https://github.com/zai-org/glm-simple-evals 上开源了一个评估工具包，以确保基准测试结果的可重复性。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/581b815696814f17931f3e02ab0dbe1a.png)

# 2. Pre-Training
## 2.1 Architecture

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/79b65132e43f41e5bc342a99fdc72585.png)

在 GLM-4.5 系列中，我们采用了 MoE 架构，从而提升了训练和推理的计算效率。我们采用无损平衡路由和 Sigmoid 门控作为 MoE 层。与 DeepSeek-V3 和 Kimi K2 不同，我们降低了模型的宽度（隐藏层维度和路由专家数量），并增加了模型的高度（层数），因为我们发现更深的模型展现出更佳的推理能力。在自注意力组件中，我们采用了带有部分 RoPE 的分组查询注意力机制 (Grouped-Query Attention)。此外，我们使用了 2.5 倍的注意力头（5120 个隐藏层维度对应 96 个注意力头）。与直觉相反的是，虽然与注意力头较少的模型相比，增加注意力头数量并没有改善训练损失，但它在 MMLU 和 BBH 等推理基准测试中持续提升了性能。我们还加入了 QK-Norm 来稳定注意力的范围。对于 GLM-4.5 和 GLM-4.5-Air，我们都添加了一个 MoE 层作为 MTP（多 token 预测）层，以支持推理过程中的推测解码。

## 2.2 Pre-Training Data

我们的预训练语料库涵盖来自网页、社交媒体、书籍、论文和代码库的文档。我们针对不同来源精心设计了数据处理流程。

**Web**。我们的预训练文档主要为从互联网上爬取的中英文网页。受 Nemotron-CC 的启发，我们将爬取到的网页按质量得分分成不同的桶。我们从质量得分较高的桶中对文档进行上采样，并丢弃质量得分最低的桶中的文档。质量得分最高的桶在预训练期间贡献了超过 3.2 个 epoch。**通过这种方式，预训练语料库可以突出推理任务的高频知识，并提高对长尾世界知识的覆盖率**。我们还发现了大量由模板自动生成并被赋予高分的相似网页。这类网页无法通过 MinHash 去重技术去除。我们还应用了 SemDedup 流程，根据文档嵌入去除这些相似的网页。

**Multilingual**。为了支持更多自然语言，我们在预训练语料库中包含了多语言文档。该多语言语料库来自我们爬取的网页和 Fineweb-2。我们应用了一个质量分类器来判断文档的教育效用，并对高质量的多语言文档进行上采样。

**Code**。我们从 GitHub 和各种代码托管平台收集了源代码数据。代码语料库首先经过基于规则的初步筛选，然后使用特定于语言的质量模型进行分类，将样本分为三个等级：高质量、中等质量和低质量。在训练过程中，我们对高质量代码进行了上采样，同时排除了低质量样本。此外，**我们将“中间填充”训练目标应用于所有源代码数据**。对于与代码相关的网页文档，我们采用两阶段检索流程，从文本预训练语料库中检索。最初，我们根据两个标准选择文档：是否存在 HTML 代码标签，或使用经过训练可检测代码相关内容的 FastText 分类器进行识别。随后，使用专用模型对检索到的文档进行质量评估，并遵循与源代码相同的基于质量的采样策略，将文档分为高、中、低质量类别。最后，使用细粒度解析器重新解析选定的网页，以更好地保留代码的格式和内容。

**Math & Science**。为了增强推理能力，我们从网页、书籍和论文中收集了与数学和科学相关的文档。我们应用一个大语言模型，根据数学和科学教育内容的比例对候选文档进行评分，并训练一个小规模的分类器来预测分数。预训练语料库中分数超过一定阈值的文档会被上采样。

GLM-4.5 的预训练过程分为两个阶段。第一阶段，模型主要在网页中的常规文档上进行训练。第二阶段，我们对来自 GitHub 的源代码以及与编程、数学和科学相关的网页进行上采样。

## 2.3 Mid-Training: Boost Reasoning & Agentic Capacity

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8fd31265cf62486aa5aa25df87c5e1e4.png)

预训练完成后，我们添加了多个阶段，以进一步提升模型在重要应用领域的性能。**与传统的基于大规模通用文档的预训练不同，这些训练阶段使用中等规模的特定领域数据集，包括指令数据**。因此，我们将这些训练阶段称为“中期训练”，其内容如下。

**Repo-level Code Training**。在此训练阶段，我们添加来自同一存储库的串联代码文件，以学习跨文件依赖关系。为了提升模型的软件工程能力，我们还添加了来自 GitHub 的经过模型过滤的问题、拉取请求 (PR) 和提交，并将相关的问题、PR 和提交串联到一个上下文中，并以类似 diff 的格式组织提交。我们将训练序列长度从 4K 扩展到 32K，以适应大型存储库。

**Synthetic Reasoning Data Training**。在这个阶段，我们为数学、科学和编程竞赛添加了综合推理内容。我们从网页和书籍中收集了大量与推理任务相关的问题和答案，并使用推理模型来合成推理过程。

**Long-context & Agent Training**。为了进一步提升模型的长上下文性能，我们将训练序列长度从 32K 扩展到 128K，并从预训练语料库中对长文档进行上采样。在此阶段，我们还加入了大规模合成 Agent 轨迹。

图 3 展示了预训练和中期训练的完整阶段。预训练阶段的最大序列长度保持在 4,096，中期训练阶段则从 32,768 扩展到 131,072。在预训练阶段，我们没有使用最佳拟合打包，因为随机截断是预训练文档的一种良好的数据增强策略。对于训练中阶段的数据集，我们采用了最佳拟合打包，以避免截断推理过程或 repo 级别的代码。

## 2.4 Hyper-Parameters

我们对除词向量、偏差和 RMSNorm 权重之外的所有参数都使用了 Muon 优化器。对于超参数，我们将 Newton-Schulz 迭代步数 N 设置为 5，动量 µ 设置为 0.95，并将 Muon 的更新 RMS 缩放为 0.2。我们观察到，Muon 优化器可以加速收敛并容忍更大的 batch size。我们使用余弦衰减策略来设置学习率，而不是预热-稳定-衰减 (WSD) 策略。我们早期的实验表明，使用 WSD 策略训练的模型在通用基准测试（SimpleQA、MMLU）上的表现较差，表明在稳定阶段存在欠拟合。学习率经历了从 0 到 2.5e-4 的预热阶段和衰减到 2.5e-5 的阶段，直到中期训练结束。我们采用了批量大小预热策略，在训练前 500B 个 token 时，批量大小从 16M 个 token 逐渐增加到 64M 个 tpken，并在剩余的训练中保持不变。对于正则化，我们将权重衰减率设置为 0.1，并且没有使用 dropout。如图 3 所示，我们在预训练期间将最大序列长度设置为 4,096，并在中期训练将其扩展至 32,768 和 131,072。在将序列长度扩展至 32K 时，我们还将 RoPE 的基频从 10,000 调整为 1,000,000，以获得更好的长上下文建模能力。对于无损失平衡路由，我们将前 15T 个 token 的偏差更新率设置为 0.001，将剩余 token 的偏差更新率设置为 0.0。我们还应用了权重为 0.0001 的辅助序列级平衡损失，以避免任何单个序列中出现极端不平衡。对于前 15T 个 token，MTP 损失权重λ设置为 0.3，对于剩余的 token，MTP 损失权重λ设置为 0.1。

# 3.Post-Training: Expert Model Iteration

我们将后训练流程分为两个不同的阶段。在第一阶段（专家训练），我们构建了专注于三个领域的专家模型：推理、代理和通用聊天。在第二阶段（统一训练），我们采用自我蒸馏技术整合多位专家，最终交付一个能够通过深思熟虑的推理和直接响应模式生成响应的综合模型。

## 3.1 Supervised Fine-Tuning

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b9bb4549dceb42c9915c40c69c706d2e.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/efc59cb1fa2d4c08b014cfd5a2a9936a.png)

我们在第一阶段（专家训练）和第二阶段（统一训练）的开始阶段都进行了有监督微调 (SFT)。在专家训练阶段，SFT 的主要作用是提供冷启动，赋予模型基本的聊天、推理和工具使用能力，这些能力可以在后续的专家强化学习训练中得到进一步增强，从而提升性能。在统一训练阶段，SFT 的目的是将不同专家模型的功能蒸馏成一个能够处理不同类型任务的混合推理通才模型。

**Cold Start SFT**。在冷启动阶段，我们利用一小组带有长思维链 (CoT) 响应的有监督微调 (SFT) 数据。这种方法确保每个专家模型在强化学习阶段之前具备足够的基础能力。

**Overall SFT**。在整体 SFT 阶段，我们从先前训练好的专家模型中收集了数百万个样本，涵盖推理任务（数学、代码、科学等）、一般聊天（写作、翻译、总结、闲聊等）、Agent 任务（基本工具使用、特别是针对真实项目开发的编码能力等）以及长上下文理解任务，并使用最大上下文长度为 128K 个 token 来训练基础模型。通过从不同专家的输出中蒸馏，该模型学会了针对每项任务应用最有效的长 CoT 推理，从而得出准确的答案。尤其要指出的是，考虑到对于某些需要快速响应的领域（例如闲聊），长时间的思考过程并非必要，我们精心平衡了包含完整推理的训练数据和缺乏明确思维过程的数据。这种方法使模型能够在反思模式和即时响应模式下运行，从而创建了一个混合推理模型。此外，我们发现以下策略有助于准备 SFT 数据以获得最佳性能。

**Reducing Character Escaping in Function Call Templates**。尽管在当前实现中，函数调用参数主要以 JSON 格式表示，但当这些参数包含代码段时，就会出现一个重大挑战。在这种情况下，代码中相当一部分字符需要转义，迫使模型生成大量的转义字符，从而增加了模型的学习负担。虽然这个问题对于主要用于通用聊天的模型来说并不重要，但对于以函数调用为核心功能的 Agent 基础模型来说，却是一个不小的挑战。为了缓解这一限制，我们提出了一种新的函数调用模板，它将函数调用的键和值封装在类似 XML 的特殊标记中。这种方法大大减少了代码段中字符转义的必要性，因为绝大多数代码都可以以其原生形式表示而无需转义。实验结果表明，所提出的函数调用模板在减少转义的同时，不会影响函数调用执行的性能。以下示例（图 4）展示了我们提出的函数调用模板的结构。详细的代码实现可以在我们的开源代码库中找到。

**Rejection Sampling**。从专家模型中采样时，我们采用全面的多阶段过滤流程，包括：（1）删除重复、过短或截断的样本，以及不符合有效推理格式的样本；（2）对具有客观答案的样本进行正确性验证；（3）利用奖赏模型过滤对主观问题的回答；（4）对于工具调用场景，确保遵守正确的工具调用协议并验证轨迹是否达到预期的最终状态。

**Prompt Selection and Response-Level Scaling**。事实证明，筛选具有挑战性的提示并对其进行响应缩放是有效的。我们尝试根据响应长度移除排名后 50% 的提示，结果发现，尽管训练数据只有一半，但数学和科学任务的成绩却提高了 2%-4%。值得注意的是，我们发现，对这些高难度提示应用响应缩放可以带来进一步的提升。**为每个提示生成四个响应，可以额外带来 1%-2% 的提升**。

**Automatic Agentic SFT Data Construction**。Agent SFT数据的构建包含四个步骤：1. **Agent 框架与工具收集**：我们收集一组 Agent 框架、现实世界的工具API和MCP服务器，同时利用LLM自动构建和模拟一批工具。2. **任务合成**：基于这些框架和工具，我们自动合成一系列 Agent 任务。一方面，对于相对成熟的框架，我们利用LLM理解其功能并自动生成相关的查询或任务。另一方面，对于较为零散或差异较大的工具，我们首先选择一个具有代表性的子集，并同样使用LLM构建关于该子集的任务。这些任务涵盖单步和多步工具调用场景。3. **轨迹生成**：对于每个合成的任务，我们利用现有的LLM生成工具调用轨迹。此外，通过使用LLM作为用户模拟器，将多步工具调用任务转换为涉及多轮对话的轨迹。 4. **质量过滤**：对于每条轨迹，使用多个评判 Agent 来评估任务是否完成。仅保留成功的轨迹。

## 3.2 Reasoning RL

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/51dff7aa13ff4f5e9c89772c0bef1bf7.png)

推理强化学习 (Reasoning RL) 专注于增强模型在需要逻辑推理、结构化问题解决和**可验证准确性**领域的能力。这包括数学、代码生成和科学推理等关键领域。这些任务的一个显著特征是其奖赏信号的高精度，因为正确性通常可以通过编程或客观清晰的方式确定。掌握这些领域不仅对于提升模型的原始智能至关重要，而且也是构建更复杂、多步骤 Agent 行为的基础。认识到推理强化学习中独特的挑战和机遇，我们开发了一套专门的技术来有效地训练我们的模型。这些方法（详见下文）旨在解决训练效率、样本多样性和数据质量等问题。我们的整体强化学习算法基于 GRPO 框架，但不包括 KL 损失项。本节中显示的比较曲线基于我们规模较小的实验模型，而非 GLM-4.5。

**Difficulty-based Curriculum Learning**。在强化学习过程中，模型的熟练程度会不断提升，导致其与静态训练数据不匹配。在后期阶段，随着模型能力的提升，过于简单的数据可能会导致所有奖赏都为 1 的批次结果。相反，在早期阶段，过于困难的数据通常会导致所有奖励都为 0 的批次结果。在这两种情况下，奖赏方差的缺乏都无法提供有用的梯度信号，严重影响训练效率。为了应对这一挑战，我们采用了基于难度的两阶段强化学习课程。该策略以及下文讨论的其他策略的有效性已通过在一个较小模型上进行的受控实验得到验证，从而可以进行快速迭代和精确的消融研究。如图 5 所示，这种两阶段方法使模型能够持续超越其性能上限。至关重要的是，为了保持较高的信号质量并降低噪声，第二阶段使用的所有问题都严格来自已验证正确答案的答案池。

**Single-Stage RL at 64K Output Length**。先前的研究建议分多个阶段进行强化学习，并逐步增加最大输出长度。然而，我们的实验表明，这种多阶段方法的效果不如直接在 64K 最大目标长度下进行的单阶段强化学习过程。由于有初始监督微调 (SFT) 已使模型适应生成 64K 长度的响应，**因此引入长度较短的强化学习阶段可能会导致模型“忘记”其长上下文能力**。这通常会导致性能显著且不可逆的下降，因为模型的平均输出长度会缩短。这种性能下降在最终的 64K 长度强化学习阶段很难恢复，从而限制了进一步的改进。我们的实验证实了这一观察结果：如图 6 所示，直接在完整的 64K 长度下应用强化学习可以不断突破模型的极限，并获得更佳的性能。

**Dynamic Sampling Temperature**。在强化学习中，采样温度是控制轨迹多样性的关键参数。温度过低会导致输出收敛，探索性不足；温度过高则会引入低质量、噪声样本，从而降低模型准确率和训练效率。**使用固定采样温度并非最优方案，因为它无法适应策略分布变得更加集中（即熵值较低）的情况，这常常导致后期探索不足**。因此，我们建议动态调整采样温度，以在准确率和探索性之间保持良好的平衡。具体而言，当rollouts的平均奖励趋于稳定时，我们将其标识为收敛阶段，并提高采样温度以促进多样性。为了降低引入过多噪声的风险，我们实施了一种质量控制机制：我们会定期在保留的验证集上，在一系列温度下评估模型的性能。然后将下一训练阶段的温度设置为最大值，该值不会导致性能相对于当前最优值下降超过1%。

**Code and Science RL**。与数学相比，编码和科学领域的强化学习在文献中受到的关注较少。我们在这些领域进行了大量的受控强化学习实验，并得出以下经验结论。对于编码强化学习，我们发现损失函数的选择对训练效率至关重要。如图 7（左）所示，与传统的序列均值损失相比，采用 token 加权均值损失非常有益。token 加权方法提供了更细粒度、更稳定的梯度信号，从而显著加快了收敛速度。该方法还有助于缓解序列级奖赏固有的长度偏差，并有效抑制训练过程中过于简单或重复的“基准”样本的生成。对于科学强化学习，我们在 GPQA-Diamond 基准测试中的发现强调了数据质量和类型是至关重要的因素。如图 7（右）所示，仅使用专家验证的多项选择题进行强化学习，与使用质量参差不齐或未经验证的数据进行训练相比，其性能显著提升。这一结果强调，即使对于像多项选择题这样格式简单的任务，严格过滤 RL 数据池以仅包含高质量、具有挑战性的实例对于有效改进模型也至关重要。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1e0dbcd36bf34ae5872b225d7af81b34.png)

## 3.3 Agentic RL

基于人类反馈的强化学习 (RLHF) 可以帮助语言模型更忠实地遵循人类指令。将强化学习应用于数学和编程竞赛，进一步揭示了强大的推理能力和良好的扩展行为，尤其是在结果可客观验证的任务上。基于这些洞察，我们专注于 Agent 环境——特别是网络搜索和代码生成 Agent ——在这些环境中，每个操作或答案都可以自动检查。这种内置的可验证性提供了密集、可靠的奖励，使我们能够更有效地扩展强化学习训练。

### 3.3.1 Data Collection and Synthesis for Agents

针对网络搜索任务和开放域信息搜索，我们开发了一套数据合成流程，用于生成需要跨多个网络源进行多步推理的复杂问答对。该语料库旨在提升 GLM 发现互联网上难以捉摸、相互交织的事实的能力。数据集构建融合了两种方法：(1) 基于知识图谱的多跳推理自动化流程；(2) 从多个网页中提取并选择性混淆内容，以准备强化学习训练信号。

对于软件工程任务，我们精心挑选了大量 GitHub 拉取请求和问题，以创建包含用户提示和可执行单元测试的实用软件开发基准。所有评估均在强化沙盒和分布式系统中运行，该系统既提供水平可扩展性，又提供强大的隔离保证。

### 3.3.2 Pushing the Limits with Reinforcement Learning and Iterative Self-distillation

我们采用分组策略优化算法进行强化学习训练。对于每个问题 $x$，我们从先前的策略 $π_{old}$ 中采样 $K$ 个 Agent 轨迹 $\{y_1, ..., y_k\}$，并针对以下目标优化模型 $π_θ$：

$$L_{RL}(\theta)=\mathbb E_{x\sim\mathcal D}[\frac{1}{K}\sum^K_{i=1}(r(x,y_i)-\bar r(x))],$$

其中 $\bar r(x) = \frac{1}{k}\sum^k_{i=1}r(x,y_i)$ 是采样响应的平均奖赏。需要注意的是，优化过程中仅使用模型生成的 token，在损失计算中忽略环境反馈。

**Outcome Supervision with Process Action Format Penalty**。对于网页搜索任务，我们将最终答案的准确率作为整个 Agent 轨迹的奖赏。对于编码 Agent，我们主要利用带有可验证测试用例的 SWE 数据进行强化学习训练。我们的实验表明，在网页搜索和 SWE 任务上进行强化学习训练，可以提升其他任务和基准测试（例如通用工具使用和 Terminal-Bench 等编码任务）的普遍性能。此外，我们应用了流程格式惩罚，以确保模型生成正确的工具调用格式。如果模型在 Agent 轨迹生成过程中未能生成正确的工具格式，则该流程将停止，并且轨迹将获得零奖赏。

**Iterative Distillation**。由于强化学习在 Agent 任务上的训练非常耗时，我们采用一种自蒸馏方法，以迭代方式提升 SFT 冷启动模型的性能，然后再在此改进的模型上恢复强化学习训练。具体来说，我们首先对初始冷启动模型进行强化学习训练，以提升 Agent 性能。当训练达到一定步数或达到稳定水平时，我们会应用自蒸馏方法，用强化学习训练模型生成的响应替换原始冷启动数据，从而创建一个更优秀的 SFT 模型。然后，我们会在此增强模型上进行进一步的强化学习训练，逐步增加训练难度。这种迭代策略使我们能够有效地突破强化学习训练模型的性能极限。

**Scaling Test-time Compute via Interaction Turns**。对于 Agent 任务，我们观察到随着与环境交互次数的增加，性能显著提升。与推理模型中对输出 token 进行扩展的测试时扩展相比，Agent 任务通过与环境持续交互来利用测试时计算，例如，在网络上四处搜索难以找到的信息，或编写测试用例进行编码任务的自我验证和自我纠正。图 8 显示，随着浏览努力程度的变化，准确率与测试时计算量呈平稳增长。

## 3.4 General RL

通用强化学习旨在全面提升模型的整体性能，修复潜在问题，并增强关键能力。我们方法的核心是一个**多源反馈**系统，该系统能够协同基于规则的反馈、人工反馈 (RLHF) 和基于模型的反馈 (RLAIF)。这个混合框架提供了更强大的训练信号，并使我们能够充分利用每个反馈源的独特优势：自动规则的精确性、人工注释者的细致判断以及人工智能驱动评估的可扩展性。

**Holistic RL**。整体强化学习的目标是在各个领域实现广泛的性能提升。为此，我们首先构建了一个包含约 5,000 个提示的平衡数据集，涵盖 7 个主要类别、33 个次要类别和 139 个第三类别。整体强化学习的奖赏信号源自人类和人工智能的反馈。对于人类反馈，我们基于偏好标注训练奖赏模型。标注者会比较模型响应，并根据对指令遵循度、安全性和事实正确性等多个维度的综合评估来分配偏好标签。对于模型反馈，我们设计了单独的评分标准，具体取决于提示是否具有客观的真实答案。将这两种反馈来源合并在一起，可以产生更可靠、更具表现力的奖励信号，从而减轻每种方法固有的局限性。

**Instruction Following RL**。指令遵循强化学习 (RL) 提升了模型理解和满足复杂指令的能力。为此，我们创建了一个细粒度的分类法，包含 7 个主要约束类型和 151 个次要约束类型，涵盖内容要求、格式规则等。基于此分类法，我们构建了一个专门的具有挑战性的指令训练集，以涵盖每种约束类型。反馈系统由确定性验证规则、经过训练的奖赏模型和评估模型组成。**这种混合反馈系统的鲁棒性在 GRPO 训练过程中至关重要**。我们观察到奖赏黑客攻击有所缓解，这使得策略模型能够在指令遵循方面实现持续稳定的改进，如图 9 所示。

**Function Calling RL**。函数调用强化学习分为基于规则的逐步强化学习和端到端多轮强化学习。由于基于规则的逐步强化学习与端到端多轮强化学习的输出长度和收敛速度相似，我们将它们直接纳入我们的通用强化学习框架。对于端到端多轮强化学习，我们首先训练专门的专家模型，然后将这些专家模型提炼到主模型中。
- **Step-wise Rule-based RL**：对于工具调用流程清晰的任务，我们会在训练数据中标注每个步骤/轮次的真实函数调用。给定任务以及之前步骤/轮次的函数调用，训练模型生成下一个 assistant 响应，该响应可以是函数调用，也可以是对用户的响应。我们使用基于规则的奖励机制，引导模型在连续的回合中正确调用函数。因此，我们设计了以下严格的奖赏函数：
$$Reward=\begin{cases}
1, & if~FormatCorrect(a_t)~and~Match(a_t,a^*_t)\\
0, & otherwise
\end{cases}$$
其中，$a_t$ 表示模型生成的第 $t$ 个函数调用，$a^∗_t$ 表示对应的真实值函数调用。只有当 $a_t$ 格式正确且与真实值完全匹配（包括名称、参数和所有字段）时，才会给予奖励 1。否则，奖励为 0。如此严格的奖励规则不仅可以引导模型生成正确的函数调用，还能有效强制输出格式，从而提升模型在实际交互中的可用性和鲁棒性。
- **End-to-end Multi-turn RL**：基于规则的分步强化学习将任务分解为静态的、预先确定的决策流。在此过程中，**模型缺乏与环境的动态交互，无法自主探索、规划或处理复杂情况，从而限制了其在现实世界中解决问题的能力**。为了解决这些问题，我们引入了端到端多轮函数调用强化学习，模型首先生成完整的轨迹，然后根据任务完成情况获得奖励。通过这种方式，模型可以通过工具反馈的不断试错来优化其行动策略，从而显著提升其自主规划和决策能力。具体而言，端到端多轮函数调用强化学习考虑两种类型的复杂任务：1. *单轮多步骤任务*：模型需要进行多步骤函数调用并与环境交互才能完成此类任务。我们使用基于多轮函数调用服务器自动合成的复杂任务，以及一些具有可运行环境的开源 Agent 数据集，例如 Agentgym。 2. *多轮多步骤任务*：除了与工具执行环境交互之外，模型还需要与 LLM 模拟的用户 Agent 交互，以获取完整的任务信息并完成整体任务。端到端多轮函数调用强化学习的奖赏计算如下：
$$Reward=\begin{cases}
1, & if~FormatCorrect(a_1,...,a_T)~and~TaskCompleted(I,o_0,a_1,o_1,...,a_T,o_T)\\
0, & otherwise
\end{cases}$$
其中，$I$ 表示原始复杂任务，$a_t$ 表示第 $t$ 次函数调用，$o_t$ 表示工具反馈或用户信息。$TaskCompleted(I, o_0, a_1, o_1, . . ., a_T, o_T)$ 表示任务是否完成，由环境根据预定义规则判断，或由 LLM Agent 判断。

**Pathology RL**。作为后训练的最后阶段，通用强化学习需要纠正潜在的问题，例如语言混合、过度重复和格式错误。虽然在上述通用强化学习任务中惩罚此类行为是有效的，但这些病态行为的发生率较低（通常不到输出的 1%），因此这是一种样本效率低下的优化策略。**因此，我们通过识别极有可能触发这些病态行为的提示，为病态强化学习精心策划了一个目标数据集**。基于该数据集进行训练使我们能够施加有效的惩罚，从而进一步降低这些问题行为的残差错误率。

## 3.5 RL Infrastructure

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dff3363e2f784a5fafb34cc732a3d002.png)

我们的强化学习基础设施基于我们开发的开源框架 Slime 构建。该框架经过多项关键优化，以增强灵活性、效率和可扩展性。

# 4.Evaluation