论文链接：https://arxiv.org/pdf/2103.00020

代码链接：https://github.com/OpenAI/CLIP

# 摘要

目前最先进的计算机视觉系统训练的目的是预测一组提前预定义的固定的类别对象。这种受限的监督形式限制了它们的通用性和可用性，因为需要额外的标注数据来指定任何其他视觉概念。直接从原始文本中学习图像是一种很有前景的替代方案，它可以利用更广泛的监督来源。我们证明了，**预测哪个标题与哪个图像匹配的简单预训练任务是一种高效且可扩展的方法**，可以从互联网上收集的 4 亿对（图像、文本）数据集上从头开始学习 SOTA 图像表征。预训练后，使用自然语言引用已学习的视觉概念（或描述新的视觉概念），从而实现模型向下游任务的**零样本迁移**。我们通过对 30 多个不同的现有计算机视觉数据集进行基准测试来研究该方法的性能，这些数据集涵盖了 OCR、视频中的动作识别、地理定位以及多种类型的细粒度对象分类等任务。该模型可以轻易地迁移到大多数任务，并且通常可以与完全监督的基线模型相媲美，而无需任何针对特定数据集的训练。例如，我们在 ImageNet 零样本训练集上达到了与原始 ResNet-50 相当的准确率，而无需使用其训练所用的 128 万个训练样本中的任何一个。我们的代码和预训练模型权重已发布在 https://github.com/OpenAI/CLIP。

# 1.Introduction and Motivating Work
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8b49515a5a324f82a33386f2bc3d8308.png)

过去几年，直接从原始文本中学习的预训练方法彻底改变了自然语言处理 (NLP)。诸如自回归和 MASK 语言模型等任务无关的目标模型在计算能力、模型容量和数据方面实现了多个数量级的扩展，能力稳步提升。“文本到文本”作为标准化输入输出接口的发展，使得任务无关的架构能够零样本迁移到下游数据集，无需专门的输出头或针对特定数据集进行定制。像 GPT-3 这样的旗舰系统如今凭借定制模型在众多任务中都具有竞争力，而且几乎不需要数据集特定的训练数据。

这些结果表明，现代预训练方法在网络规模文本数据集上所能达到的总体监督效果，已经超越了高质量众包标注的自然语言处理 (NLP) 数据集。然而，在计算机视觉等其他领域，在 ImageNet 等众包标注数据集上预训练模型仍然是标准做法。直接从网络文本中学习的可扩展预训练方法能否在计算机视觉领域带来类似的突破？先前的研究成果令人鼓舞。

二十多年前，Mori et al. (1999) 通过训练模型来预测与图像配对的文本文档中的名词和形容词，探索了改进基于内容的图像检索。Quattoni et al. (2007) 证明，通过在训练用于预测与图像相关的标题中的单词的分类器的权重空间中进行流形学习，可以学习到数据效率更高的图像表示。Srivastava & Salakhutdinov (2012) 通过在低级图像和文本标签特征之上训练多模态深度玻尔兹曼机，探索了深度表示学习。Joulin et al. (2016) 对这项工作进行了现代化改造，并证明经过训练用于预测图像标题中单词的 CNN 可以学习有用的图像表示。他们将 YFCC100M 数据集中图片的标题、描述和主题标签元数据转换为词袋多标签分类任务，并表明使用 AlexNet 进行预训练来预测这些标签可以学习到与基于 ImageNet 的预训练在迁移任务上的表现类似的表征。Li et al. (2017) 随后将这种方法扩展到除了单个单词之外还可以预测短语 n-gram，并展示了他们的系统能够通过基于学习到的视觉 n-gram 词典对目标类别进行评分并预测得分最高的类别，从而对其他图像分类数据集进行零样本迁移。VirTex、ICMLM 和 ConVIRT 采用了更新的架构和预训练方法，最近展示了基于 Transformer 的语言模型、MASK 语言模型和对比目标在从文本中学习图像表征方面的潜力。

虽然作为概念证明令人兴奋，但使用自然语言监督进行图像表示学习仍然很少见。这可能是因为在常见基准上展示的性能远低于替代方法。例如，Li et al. (2017) 在零样本设置下在 ImageNet 上仅达到 11.5% 的准确率。这远低于当前最先进技术的 88.4% 的准确率。它甚至低于经典计算机视觉方法 50% 的准确率。相反，范围更窄但针对性更强的弱监督使用可以提高性能。Mahajan et al. (2018) 表明，预测 Instagram 图片上与 ImageNet 相关的主题标签是一项有效的预训练任务。当针对 ImageNet 进行微调时，这些预训练模型的准确率提高了 5% 以上，并提高了当时的整体水平。Kolesnikov et al. (2019) 和 Dosovitskiy et al. (2020) 还通过预训练模型来预测噪声标记的 JFT-300M 数据集的类别，在更广泛的迁移基准上取得了巨大的进步。

这类研究代表了当前应用的中间地带，介于从有限数量的监督“golden 标签”学习和从几乎无限量的原始文本学习之间。然而，这并非没有妥协。这两项研究都经过精心设计，并在过程中分别将其监督限制在 1000 个和 18291 个类别。自然语言能够通过其通用性来表达更广泛的视觉概念，并因此对其进行监督。这两种方法都使用静态 Softmax 分类器进行预测，并且缺乏动态输出机制。这严重削弱了它们的灵活性，并限制了它们的“零样本”能力。

这些弱监督模型与近期直接从自然语言学习图像表征的探索之间的一个关键区别在于规模。Mahajan et al. (2018) 和  Kolesnikov et al. (2019) 的模型在数百万到数十亿张图像上进行了“**加速器年**”的训练，而 VirTex、ICMLM 和 ConVIRT 则在一到二十万张图像上进行了“**加速器日**”的训练。在本研究中，我们缩小了这一差距，并研究了在大规模自然语言监督下训练的图像分类器的行为。借助互联网上大量此类公开数据，我们创建了一个包含 4 亿对（图像、文本）的新数据集，并证明了一个从零开始训练的 ConVIRT 的简化版本（我们称之为 **CLIP**，即 Contrastive Language-Image Pre-training）是一种有效的自然语言监督学习方法。我们通过训练一系列八个模型（涵盖近两个数量级的计算量）来研究 CLIP 的可扩展性，并观察到迁移性能是一个可平滑预测的计算函数。我们发现，CLIP 与 GPT 家族类似，在预训练阶段就能学会执行一系列广泛的任务，包括 OCR、地理定位、动作识别等等。我们通过在 30 多个现有数据集上对 CLIP 的零样本迁移性能进行基准测试来衡量这一点，发现它可以与之前针对特定任务的监督模型相媲美。我们还通过线性探测表征学习分析证实了这些发现，并表明 CLIP 的性能优于目前最好的公开 ImageNet 模型，同时计算效率更高。此外，我们还发现零样本 CLIP 模型比同等准确率的监督 ImageNet 模型更加稳健，这表明对与任务无关的模型进行零样本评估更能体现模型的能力。这些结果具有重要的政策和伦理意义，我们将在第 7 节中对此进行探讨。

# 2.Approach

## 2.1 Natural Language Supervision

**我们的方法的核心来自于从自然语言的有监督学习中学习感知的思想**。正如引言中所讨论的，这并非一个全新的理念，然而，描述该领域工作的术语多种多样，甚至看似相互矛盾，而且所阐述的动机也各不相同。Zhang et al. (2020), Gomez et al. (2017), Joulin et al. (2016) 以及 Desai & Johnson (2020) 都介绍了从文本与图像对中学习视觉表征的方法，但分别将其方法描述为无监督、自监督、弱监督和有监督。

我们强调，这一研究领域的共同点并非在于所使用的具体方法的细节，而是**在于对自然语言作为训练信号的理解**。所有这些方法都基于自然语言有监督进行学习。尽管早期研究在使用主题模型和 n-gram 表示时，难以应对自然语言的复杂性，但深度表示学习的进步表明，我们现在拥有了有效利用这种丰富有监督资源的工具。

与其他训练方法相比，自然语言学习具有诸多潜在优势。与用于图像分类的标准众包标注相比，自然语言监督标注更容易扩展，因为它不需要标注采用经典的“机器学习兼容格式”，例如规范的 1/N 多数投票“gold 标签”。相反，基于自然语言的方法可以从互联网上海量文本的监督学习中被动学习。与大多数无监督或自监督学习方法相比，自然语言学习还有一个重要优势，即它并非“仅仅”学习一种表征，而是将该表征与语言联系起来，从而实现灵活的零样本迁移。在以下小节中，我们将详细介绍我们最终确定的具体方法。

## 2.2 Creating a Sufficiently Large Dataset

现有工作主要使用三个数据集：MS-COCO、Visual Genome 和 YFCC100M。虽然 MS-COCO 和 Visual Genome 是高质量的众包标记数据集，但按照现代标准，它们的规模较小，每个数据集大约包含 10 万张训练照片。相比之下，其他计算机视觉系统则使用多达 35 亿张 Instagram 照片进行训练。包含 1 亿张照片的 YFCC100M 是一个可行的替代方案，但每张图片的元数据稀疏且质量参差不齐。许多图像使用自动生成的文件名（如 20160716 113957.JPG）作为“标题”或包含相机曝光设置的“描述”。经过筛选，仅保留具有自然语言标题和/或英文描述的图像后，数据集缩小了 6 倍，只有 1500 万张照片。这大约与 ImageNet 的大小相同。

自然语言监督的一个主要动机是互联网上有大量此类数据可供公开获取。由于现有数据集未能充分反映这种可能性，仅考虑现有数据集的结果会低估这一研究领域的潜力。为了解决这个问题，我们构建了一个包含 4 亿对（图像，文本）的新数据集，这些对来自互联网上各种公开来源。为了尽可能广泛地涵盖视觉概念，我们在构建过程中通过 query 搜索（图像，文本）对，其文本包含 50 万个 query 中的一个（基础 query 列表包含所有在维基百科英文版中至少出现 100 次的单词。此外，query 列表还添加了具有高逐点互信息量的二元词组，以及所有搜索量超过一定水平的维基百科文章名称。最后，所有尚未包含在 query 列表中的 WordNet 同义词集均被添加。）。我们通过为每个 query 包含最多 2 万对（图像，文本）对来粗略地平衡结果的类别。生成的数据集的总字数与用于训练 GPT-2 的 WebText 数据集相似。我们将此数据集称为 WIT（WebImageText）。

## 2.3 Selecting an Efficient Pre-Training Method
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b1fa045d82344e2881f7db2f789526ab.png)

最先进的计算机视觉系统需要使用非常大量的计算资源。Mahajan et al. (2018) 需要 19 个 GPU 年来训练他们的 ResNeXt101-32x48d 模型，而 Xie et al. (2020) 则需要 33 个 TPUv3 核心年来训练他们的 Noisy Student EfficientNet-L2 模型。考虑到这两个系统都只训练用于预测 1000 个 ImageNet 类别，从自然语言中学习一组开放的视觉概念似乎是一项艰巨的任务。在研究过程中，我们发现训练效率是成功扩展自然语言监督模型的关键，并基于这一指标选择了最终的预训练方法。

我们最初的方法与 VirTex 类似，从零开始联合训练图像 CNN 和文本 Transformer，以预测图像的标题。然而，我们在有效扩展此方法方面遇到了困难。图 2 中展示了一个拥有 63M 参数的 Transformer 语言模型，其计算量已经是 ResNet-50 图像编码器的两倍，但学习识别 ImageNet 类别的速度却比一个简单得多的基线模型（用于预测相同文本的词袋编码）慢三倍。

这两种方法有一个关键的相似之处。它们都试图预测每幅图像所附文本的确切词语。由于图像中同时出现的描述、评论和相关文本种类繁多，这项任务非常艰巨。**近期在图像对比表征学习方面的研究发现，对比目标可以比其等效的预测目标学习到更好的表征**。其他研究发现，尽管图像生成模型可以学习高质量的图像表征，但它们所需的计算量比具有相同性能的对比模型高出一个数量级。基于这些发现，我们探索了训练一个系统来解决可能更简单的代理任务，即仅预测哪段文本作为整体与哪幅图像配对，而不是预测文本中的确切词语。从相同的词袋编码基线开始，我们将图 2 中的预测目标替换为对比目标，并观察到零样本迁移到 ImageNet 的效率进一步提高了 4 倍。

给定一批 $N$ 个（图像，文本）对，CLIP 被训练用于预测一批 $N × N$ 个可能的（图像，文本）对中哪些实际匹配。为此，CLIP 通过联合训练图像编码器和文本编码器来学习多模态嵌入空间，以最大化批次中 $N$ 个真实对的图像和文本嵌入的余弦相似度，同时最小化 $N^2 − N$ 个不正确对的嵌入的余弦相似度。我们针对这些相似度得分优化了一个**对称交叉熵损失**。图 3 中包含了 CLIP 实现核心的伪代码。据我们所知，这种批次构建技术和目标最初是在深度度量学习领域作为多类 N 对损失引入的，后来由 Oord et al. (2018) 作为 InfoNCE 损失推广用于对比表征学习，最近又被  Zhang et al. (2020) 应用于医学成像领域的对比（文本，图像）表征学习。

由于我们的预训练数据集规模庞大，过拟合并非主要问题，而且与 Zhang et al. (2020) 的实现相比，CLIP 的训练细节也得到了简化。我们从头开始训练 CLIP，**无需使用 ImageNet 权重初始化图像编码器，也无需使用预训练权重初始化文本编码器**。我们没有使用表征和对比嵌入空间之间的非线性投影，这一改变由 Bachman et al. (2019) 提出，并由 Chen et al. (2020b) 推广。**相反，我们仅使用线性投影将每个编码器的表征映射到多模态嵌入空间**。我们没有注意到两个版本之间的训练效率差异，并推测非线性投影可能仅在自监督表征学习方法中与当前图像的细节协同调整。我们还删除了 Zhang et al. (2020) 中的文本转换函数 $t_u$，该方法从文本中均匀采样单个句子，因为 CLIP 预训练数据集中的许多 (image, text) 对只有一个句子。我们还简化了图像变换函数 $t_v$。从调整大小的图像中随机裁剪一个正方形区域是训练过程中唯一使用的数据增强方法。**最后，控制 softmax 中对数函数范围的温度参数 $τ$ 在训练过程中直接优化为对数参数化的乘法标量，以避免将其转换为超参数**。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fb9ac4fc198e49729b400105d5efb6b9.png)
## 2.4 Choosing and Scaling a Model

我们考虑了两种不同的图像编码器架构。首先，我们使用 **ResNet-50** 作为图像编码器的基础架构，因为它被广泛采用且性能得到验证。我们使用 He et al. (2019) 提出的 ResNetD 改进和 Zhang (2019) 提出的抗锯齿 rect-2 模糊池化对原始版本进行了一些修改。我们还用注意力池机制替换了全局平均池化层。注意力池实现为单层 Transformer 的多头 QKV 注意力，其中 query 以图像的全局平均池化表示为条件。对于第二种架构，我们尝试了最近推出的 **Vision Transformer (ViT)**。我们紧密遵循它们的实现方式，仅做了一些细微的修改，即在 Transformer 之前为组合的 patch 和位置嵌入添加了一个额外的层正则化，并使用了略有不同的初始化方案。

文本编码器是一个 Transformer，其架构修改见  Radford et al. (2019) 的文章。我们使用一个 63M 参数、12 层、512 宽度的模型作为基础大小，该模型具有 8 个注意力头。Transformer 对文本进行小写的字节对编码 (BPE) ，词表为 49,152。为了提高计算效率，最大序列长度上限为 76。文本序列用 [SOS] 和 [EOS] token括起来，Transformer 最高层在 [EOS] token处的激活被视为文本的特征表示，该特征表示经过层归一化，然后线性投影到多模态嵌入空间。文本编码器中使用了 MASK 自注意力机制，以保留使用预训练语言模型进行初始化或添加语言建模作为辅助目标的能力，但这方面的探索留待未来研究。

虽然先前的计算机视觉研究通常通过单独增加宽度或深度来扩展模型，但对于 ResNet 图像编码器，我们采用了 Tan & Le (2019) 的方法，他们发现，在宽度、深度和分辨率上分配额外的计算能力，优于仅将其分配给模型的一个维度。Tan & Le (2019) 针对其 EfficientNet 架构调整了分配给每个维度的计算能力比例，而我们采用简单的基准方法，即平等地分配额外的计算能力以增加模型的宽度、深度和分辨率。对于文本编码器，我们仅按 ResNet 计算出的宽度增量按比例缩放模型的宽度，而不缩放深度，因为我们发现 CLIP 的性能对文本编码器的容量不太敏感。

## 2.5 Training

我们训练了一系列 5 个 ResNet 和 3 个 Vision Transformer。对于 ResNet，我们训练了一个 ResNet-50、一个 ResNet-101，然后训练了另外 3 个，它们遵循 EfficientNet 风格的模型缩放，使用的计算量约为 ResNet-50 的 4 倍、16 倍和 64 倍。它们分别表示为 RN50x4、RN50x16 和 RN50x64。对于 Vision Transformer，我们训练了一个 ViT-B/32、一个 ViT-B/16 和一个 ViT-L/14。我们对所有模型进行 32 个 epoch 的训练。我们使用 Adam 优化器，并将**解耦权重衰减正则化应用于所有非增益或偏差的权重**，并使用余弦算法衰减学习率。在训练 1 个 epoch 的基准 ResNet50 模型上，使用网格搜索、随机搜索和手动调整的组合设置初始超参数。由于计算限制，我们随后启发式地调整了超参数以适应更大的模型。可学习的温度参数 $τ$ 被初始化为相当于0.07的值，并进行了裁剪以防止对数缩放超过100（我们认为这是防止训练不稳定的必要条件）。我们使用了非常大的 32,768 的 batch-size。混合精度用于加速训练并节省内存。为了节省更多内存，我们使用了梯度检查点、半精度Adam统计量和半精度随机舍入文本编码器权重。嵌入相似度的计算也被分片，各个GPU仅计算其本地嵌入批量所需的成对相似度子集。最大的ResNet模型RN50x64在592块V100 GPU上花费18天进行训练，而最大的Vision Transformer在256块V100 GPU上花费12天进行训练。对于 ViT-L/14，我们还以更高的 336 像素分辨率预训练了一个额外的 epoch，以提高与 FixRes 类似的性能。我们将此模型表示为 ViT-L/14@336px。除非另有说明，本文中所有以“CLIP”形式报告的结果均采用我们发现性能最佳的此模型。

# 3.Experiments
