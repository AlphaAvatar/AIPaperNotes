# AIPaperNotes
Record daily reading of papers and related reproduction results in Chinese.

For more notes, please follow the blog: https://nopsled.blog.csdn.net/

## Paper Catalog

### Embedding

- NV-EMBED [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Embedding/2025/NV-EMBED%3A%20IMPROVED%20TECHNIQUES%20FOR%20TRAINING%20LLMS%20AS%20GENERALIST%20EMBEDDING%20MODELS.md)]
- Qwen3 Embedding [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Embedding/2025/Qwen3%20Embedding%3A%20Advancing%20Text%20Embedding%20and%20Reranking%20Through%20Foundation%20Models.md)]

### Flow

- Flow Matching [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Flow/2023/FLOW%20MATCHING%20FOR%20GENERATIVE%20MODELING.md)]

### LLM

- **Agent**: LLM-based Single/Multi Agent model/system
    - **DeepResearch**:
        - WebSailor [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/DeepResearch/2025/WebSailor%3A%20Navigating%20Super-human%20Reasoning%20for%20Web%20Agent.md)]
        - WebWatcher [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/DeepResearch/2025/WebWatcher%3A%20Breaking%20New%20Frontiers%20of%20Vision-Language%20Deep%20Research%20Agent.md)]
    - **Memory**:
        - Dynamic Cheatsheet [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/Memory/2025/Dynamic%20Cheatsheet%3A%20Test-Time%20Learning%20with%20Adaptive%20Memory.md)]
        - EgoMem [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/Memory/2025/EgoMem%3A%20Lifelong%20Memory%20Agent%20for%20Full-duplex%20Omnimodal%20Models.md)]
        - ReasoningBank
    - **Multi Agent Optimization**
        - OWL [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/Multi%20Agent%20Optimization/2025/OWL%3A%20Optimized%20Workforce%20Learning%20for%20General%20Multi-Agent%20Assistance%20in%20Real-World%20Task%20Automation.md)]
        - Multi-Agent Design [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/Multi%20Agent%20Optimization/2025/Multi-Agent%20Design%3A%20Optimizing%20Agents%20with%20Better%20Prompts%20and%20Topologies.md)]
    - **RAG**
        - BookRAG [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/RAG/2025/BookRAG%3A%20A%20Hierarchical%20Structure-aware%20Index-based%20Approach.md)]
    - **Reflection**:
        - Metacognitive Reuse [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/Reflection/2025/Metacognitive%20Reuse%3A%20Turning%20Recurring%20LLM%20Reasoning%20Into%20Concise%20Behaviors.md)]
    - **Router**:
        - Router-R1 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/Router/2025/Router-R1%3A%20Teaching%20LLMs%20Multi-Round%20Routing%20and%20Aggregation%20via%20Reinforcement%20Learning.md)]
    - **Visual Reasoning**
        - PixelCraft [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Agent/Visual%20Reasoning/2025/PIXELCRAFT%3A%20A%20MULTI-AGENT%20SYSTEM%20FOR%20HIGH-FIDELITY%20VISUAL%20REASONING%20ON%20STRUCTURED%20IMAGES.md)]

- **Base Model**: Large Language Model
    - **Moonshot AI**
        - KIMI LINEAR [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Base%20Model/Moonshot%20AI/2025/KIMI%20LINEAR%3A%20AN%20EXPRESSIVE%2C%20EFFICIENT%20ATTENTION%20ARCHITECTURE.md)]
    - **Zhipu AI**
        - GLM 4.5 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Base%20Model/Zhipu%20AI/2025/GLM-4.5%3A%20Agentic%2C%20Reasoning%2C%20and%20Coding%20(ARC)%20Foundation%20Models.md)]
    - **OpenAI**
        - gpt-oss [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Base%20Model/OpenAI/2025/gpt-oss-120b%20%26%20gpt-oss-20b%20Model%20Card.md)]

- **Dataset**: Data building and processing for Model training
    - DELT [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Dataset/2025/Data%20Efficacy%20for%20Language%20Model%20Training.md)]

- **Long Sequence**
    - RLM

- **Prompt**: Prompt Engineering
    - **Context Learning**
        - ACE [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Prompt/Context%20Learning/2025/Agentic%20Context%20Engineering%3A%20Evolving%20Contexts%20for%20Self-Improving%20Language%20Models.md)]

- **Omni**: LLM-based full modal model
    - Qwen2.5 - Omni [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Omni/2025/Qwen2.5-Omni%20Technical%20Report.md)]
    - M3 - Agent [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Omni/2025/Seeing%2C%20Listening%2C%20Remembering%2C%20and%20Reasoning%3A%20A%20Multimodal%20Agent%20with%20Long-Term%20Memory.md)]

- **Quantization**: Model Weight/Optimizer/Activation Compressing
    - COAT [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Quantization/2025/COAT%3A%20COMPRESSING%20OPTIMIZER%20STATES%20AND%20ACTIVATION%20FOR%20MEMORY-EFFICIENT%20FP8%20TRAINING.md)]

- **Speech**: Speech LLM
    - **ALM**: Audio LLM for auido Input
        - Audio Flamingo 3 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Speech/ALM/2025/Audio%20Flamingo%203%3A%20Advancing%20Audio%20Intelligence%20with%20Fully%20Open%20Large%20Audio%20Language%20Models.md)]

- **Survey**
    - DeepRearch [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Survey/DeepRearch/2025/A%20Comprehensive%20Survey%20of%20Deep%20Research%3A%20Systems%2C%20Methodologies%2C%20and%20Applications.md)]
    - Vibe Coding

- **Training**: LLM Model Training:
    - **Ptrtrain**
        - FIM (fill-in-the-middle) [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/Pretrain/2022/Efficient%20Training%20of%20Language%20Models%20to%20Fill%20in%20the%20Middle.md)]
    - **RL**
        - **RLHF**: reinforcement learning from human feedback
            - BCO [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/RL/RLHF/2025/Binary%20Classifier%20Optimization%20for%20Large%20Language%20Model%20Alignment.md)]
        - **RLVF**: reinforcement learning with verifiable rewards
            - Deepseek - R1 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/RL/RLVF/2025/DeepSeek-R1%3A%20Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%20Reinforcement%20Learning.md)]
            - Dr.GRPO [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/RL/RLVF/2025/Understanding%20R1-Zero-Like%20Training%3A%20A%20Critical%20Perspective.md)]
            - DAPO [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/RL/RLVF/2025/DAPO%3A%20An%20Open-Source%20LLM%20Reinforcement%20Learning%20System%20at%20Scale.md)]
            - GCG [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/RL/RLVF/2025/GPG%3A%20A%20Simple%20and%20Strong%20Reinforcement%20Learning%20Baseline%20for%20Model%20Reasoning.md)]
            - LUFFY [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/RL/RLVF/2025/Learning%20to%20Reason%20under%20Off-Policy%20Guidance.md)]
            - GSPO
            - DeepSeek - R1 v2 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/RL/RLVF/2026/DeepSeek-R1%20v2.md)]
    - **Speculative Decoding or MTP**: Speculative Decoding or Multi-token Prediction
        - Better & Faster Large Language Models via Multi-token Prediction [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/Speculative%20Decoding/2025/Better%20%26%20Faster%20Large%20Language%20Models%20via%20Multi-token%20Prediction.md)]
        - CAFT [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/Speculative%20Decoding/2025/Improving%20Large%20Language%20Models%20with%20Concept-Aware%20Fine-Tuning.md)]
        - EAGLE3 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/Training/Speculative%20Decoding/2025/EAGLE-3%3A%20Scaling%20up%20Inference%20Acceleration%20of%20Large%20Language%20Models%20via%20Training-Time%20Test.md)]

- **VLM**: Visual LLM
    - LLaVA [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/VLM/2023/Visual%20Instruction%20Tuning.md)]
    - Qwen - VL [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/VLM/2023/Qwen-VL%3A%20A%20Versatile%20Vision-Language%20Model%20for%20Understanding%2C%20Localization%2C%20Text%20Reading%2C%20and%20Beyond.md)]
    - Qwen2 - VL [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/VLM/2024/Qwen2-VL%3A%20Enhancing%20Vision-Language%20Model%E2%80%99s%20Perception%20of%20the%20World%20at%20Any%20Resolution.md)]
    - Qwen2.5 - VL [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/VLM/2025/Qwen2.5-VL%20Technical%20Report.md)]
    - Qwen3 - VL [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/VLM/2025/Qwen3-VL%20Technical%20Report.md)]
    - MiniCPM-V 4.5 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/VLM/2025/MiniCPM-V%204.5%3A%20Cooking%20Efficient%20MLLMs%20via%20Architecture%2C%20Data%2C%20and%20Training%20Recipes.md)]
    - DeepSeek - OCR [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/LLM/VLM/2025/DeepSeek-OCR%3A%20Contexts%20Optical%20Compression.md)]
    - Kimi K2.5

### Visual Encoder

- **Architecture**
    - NaViT [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Visual%20Encoder/Architecture/2023/Patch%20n%E2%80%99%20Pack%3A%20NaViT%2C%20a%20Vision%20Transformer%20for%20any%20Aspect%20Ratio%20and%20Resolution.md)]

- **Image Segment Pretraining**
    - SAM [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Visual%20Encoder/Image%20Segment%20Pretraining/2023/Segment%20Anything.md)]
    - SAM2 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Visual%20Encoder/Image%20Segment%20Pretraining/2024/SAM%202%3A%20Segment%20Anything%20in%20Images%20and%20Videos.md)]

- **Language-Image Representation Learning**:
    - CLIP [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Visual%20Encoder/Language%20Image%20Pretraining/2021/Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision.md)]
    - SigLIP [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Visual%20Encoder/Language%20Image%20Pretraining/2023/Sigmoid%20Loss%20for%20Language%20Image%20Pre-Training.md)]
    - SigLIP2 [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Visual%20Encoder/Language%20Image%20Pretraining/2025/SigLIP%202%3A%20Multilingual%20Vision-Language%20Encoders%20with%20Improved%20Semantic%20Understanding%2C%20Localization%2C%20and%20Dense%20Features.md)]
    - LIFT [[Link](https://github.com/AlphaAvatar/AIPaperNotes/blob/main/Visual%20Encoder/Language%20Image%20Pretraining/2025/Language-Image%20Alignment%20with%20Fixed%20Text%20Encoders.md)]
